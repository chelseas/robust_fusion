{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cruise Control LiDAR Problem\n",
    "In this problem, you have a lead vehicle and an ego vehicle. They are driving on a road and the ego vehicle needs to estimate the distance to the car in front of it. It had a noisy ranging sensor that returns 10 \"pings\" and the ego needs to estimate the distance to the car in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "# label: true distance to the car in front\n",
    "# training data: noisy ranging sensor returns\n",
    "n_train = 10000\n",
    "n_meas = 10\n",
    "X = []\n",
    "Y = []\n",
    "for i = 1:n_train\n",
    "    y = rand()*100 # true distance in [0,100] meters\n",
    "    # noisy measurements: Gaussian with occaisional multipath outliers\n",
    "    x = randn(n_meas) .+ y\n",
    "    if rand() < 0.2 # add an outlier to this batch of measurements \n",
    "        x[rand(1:n_meas)] *= 2\n",
    "    end\n",
    "    push!(X, x)\n",
    "    push!(Y, y)\n",
    "end\n",
    "using Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(Y, size=(150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram(X[rand(1:n_train)], size=(150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hcat(X...)\n",
    "println(\"shape of X is \", size(X))\n",
    "Y = reshape(Y, 1, n_train)\n",
    "println(\"shape of Y is \", size(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "import Flux: relu, Data.DataLoader, Losses.mse, ADAM, train!, Params, gradient, update!, logging_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(10, 10, relu),\n",
    "    Dense(10, 10, relu),\n",
    "    Dense(10, 1, relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(X, Y, batchsize=8, shuffle=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x,y) = mse(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ADAM(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local training_loss\n",
    "ps = Params(model)\n",
    "for d in train_data \n",
    "    gs = gradient(ps) do\n",
    "        training_loss = loss(d...)\n",
    "        return training_loss\n",
    "    end\n",
    "    logging_callback(training_loss)\n",
    "    update!(opt, ps, gs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_all(dataloader, model)\n",
    "    l = 0f0\n",
    "    for (x,y) in dataloader\n",
    "        l += mse(model(x), y)\n",
    "    end\n",
    "    l/length(dataloader)\n",
    "end\n",
    "evalcb = () -> @show(loss_all(train_data, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.16675152080061695\n",
      "loss_all(train_data, model) = 0.16706095358409406\n",
      "loss_all(train_data, model) = 0.16655305477178103\n",
      "loss_all(train_data, model) = 0.16618132577781858\n",
      "loss_all(train_data, model) = 0.16656256624328486\n",
      "loss_all(train_data, model) = 0.16781563660222285\n",
      "loss_all(train_data, model) = 0.17009468272328185\n",
      "loss_all(train_data, model) = 0.1728974419860046\n",
      "loss_all(train_data, model) = 0.17570372211166776\n",
      "loss_all(train_data, model) = 0.17786601984479794\n",
      "loss_all(train_data, model) = 0.17961749291338758\n",
      "loss_all(train_data, model) = 0.17957853394898257\n",
      "loss_all(train_data, model) = 0.18053404909485624\n",
      "loss_all(train_data, model) = 0.18345837105313464\n",
      "loss_all(train_data, model) = 0.1838005419086492\n",
      "loss_all(train_data, model) = 0.18359634486543316\n",
      "loss_all(train_data, model) = 0.1806550411464284\n",
      "loss_all(train_data, model) = 0.17811471469565185\n",
      "loss_all(train_data, model) = 0.174595190021005\n",
      "loss_all(train_data, model) = 0.17253310253616033\n",
      "loss_all(train_data, model) = 0.1685054140155204\n",
      "loss_all(train_data, model) = 0.16619427067250372\n",
      "loss_all(train_data, model) = 0.1654330134169332\n",
      "loss_all(train_data, model) = 0.16538974946228194\n",
      "loss_all(train_data, model) = 0.1657781232686725\n",
      "loss_all(train_data, model) = 0.16682378123879082\n",
      "loss_all(train_data, model) = 0.16745269599150991\n",
      "loss_all(train_data, model) = 0.1672426113024775\n",
      "loss_all(train_data, model) = 0.16702086334055122\n",
      "loss_all(train_data, model) = 0.16673106939546806\n",
      "loss_all(train_data, model) = 0.16647747456986442\n",
      "loss_all(train_data, model) = 0.16619134141711475\n",
      "loss_all(train_data, model) = 0.16626853581032933\n",
      "loss_all(train_data, model) = 0.16634034110579413\n",
      "loss_all(train_data, model) = 0.1667366049406539\n",
      "loss_all(train_data, model) = 0.16759232197813467\n",
      "loss_all(train_data, model) = 0.17001734510571795\n",
      "loss_all(train_data, model) = 0.1701250274489041\n",
      "loss_all(train_data, model) = 0.1691730396407671\n",
      "loss_all(train_data, model) = 0.16803410653855436\n",
      "loss_all(train_data, model) = 0.16663245914171868\n",
      "loss_all(train_data, model) = 0.1660226043129786\n",
      "loss_all(train_data, model) = 0.16606776074585736\n",
      "loss_all(train_data, model) = 0.16671312348877645\n",
      "loss_all(train_data, model) = 0.16843811827049418\n",
      "loss_all(train_data, model) = 0.17069115854494726\n",
      "loss_all(train_data, model) = 0.17339078749258166\n",
      "loss_all(train_data, model) = 0.17544804547540896\n",
      "loss_all(train_data, model) = 0.17564096531294404\n",
      "loss_all(train_data, model) = 0.17521609380626604\n",
      "loss_all(train_data, model) = 0.17471331442321122\n",
      "loss_all(train_data, model) = 0.17530509040962325\n",
      "loss_all(train_data, model) = 0.17474117290895605\n",
      "loss_all(train_data, model) = 0.1743899823802175\n",
      "loss_all(train_data, model) = 0.17335855422917054\n",
      "loss_all(train_data, model) = 0.17171120342794144\n",
      "loss_all(train_data, model) = 0.16998554507763033\n",
      "loss_all(train_data, model) = 0.16779699413363455\n",
      "loss_all(train_data, model) = 0.16624120246544208\n",
      "loss_all(train_data, model) = 0.16556184326061119\n",
      "loss_all(train_data, model) = 0.16559304170461503\n",
      "loss_all(train_data, model) = 0.1663593549496729\n",
      "loss_all(train_data, model) = 0.16773256493910882\n",
      "loss_all(train_data, model) = 0.16864646174509984\n",
      "loss_all(train_data, model) = 0.16914333706786855\n",
      "loss_all(train_data, model) = 0.17001084988709164\n",
      "loss_all(train_data, model) = 0.1687959486527887\n",
      "loss_all(train_data, model) = 0.16730154853363127\n",
      "loss_all(train_data, model) = 0.1669231962785747\n",
      "loss_all(train_data, model) = 0.16639969093945453\n",
      "loss_all(train_data, model) = 0.16613546142739094\n",
      "loss_all(train_data, model) = 0.166011448835511\n",
      "loss_all(train_data, model) = 0.16567766617959973\n",
      "loss_all(train_data, model) = 0.1655310700072405\n",
      "loss_all(train_data, model) = 0.1656231763079179\n",
      "loss_all(train_data, model) = 0.16638959838714976\n",
      "loss_all(train_data, model) = 0.1677201619344372\n",
      "loss_all(train_data, model) = 0.17042118553188637\n",
      "loss_all(train_data, model) = 0.1721556746791462\n",
      "loss_all(train_data, model) = 0.1721334898617873\n",
      "loss_all(train_data, model) = 0.17078014668012764\n",
      "loss_all(train_data, model) = 0.1680046512673734\n",
      "loss_all(train_data, model) = 0.16613024420777267\n",
      "loss_all(train_data, model) = 0.1657826729506448\n",
      "loss_all(train_data, model) = 0.1666629592809723\n",
      "loss_all(train_data, model) = 0.167423360768286\n",
      "loss_all(train_data, model) = 0.16839640404081888\n",
      "loss_all(train_data, model) = 0.1672490016128174\n",
      "loss_all(train_data, model) = 0.16659945116629424\n",
      "loss_all(train_data, model) = 0.16627824443540135\n",
      "loss_all(train_data, model) = 0.16658945273034692\n",
      "loss_all(train_data, model) = 0.1661323696338659\n",
      "loss_all(train_data, model) = 0.1660044015707352\n",
      "loss_all(train_data, model) = 0.1660938322985009\n",
      "loss_all(train_data, model) = 0.16622337406329532\n",
      "loss_all(train_data, model) = 0.1662548300421174\n",
      "loss_all(train_data, model) = 0.16628097998210167\n",
      "loss_all(train_data, model) = 0.1663402585328131\n",
      "loss_all(train_data, model) = 0.16649082161560902\n",
      "loss_all(train_data, model) = 0.1670904220325747\n",
      "loss_all(train_data, model) = 0.1680499161062907\n",
      "loss_all(train_data, model) = 0.16888122876744338\n",
      "loss_all(train_data, model) = 0.16954708786276085\n",
      "loss_all(train_data, model) = 0.1696897351906772\n",
      "loss_all(train_data, model) = 0.16924610907730483\n",
      "loss_all(train_data, model) = 0.16892950255308042\n",
      "loss_all(train_data, model) = 0.16680421279124732\n",
      "loss_all(train_data, model) = 0.16674333193761953\n",
      "loss_all(train_data, model) = 0.1665183677772652\n",
      "loss_all(train_data, model) = 0.16619952338551125\n",
      "loss_all(train_data, model) = 0.16611992836973963\n",
      "loss_all(train_data, model) = 0.16628081092947847\n",
      "loss_all(train_data, model) = 0.16635336174929855\n",
      "loss_all(train_data, model) = 0.16621854904741723\n",
      "loss_all(train_data, model) = 0.1661923538762772\n",
      "loss_all(train_data, model) = 0.16635839845429531\n",
      "loss_all(train_data, model) = 0.16655046103835913\n",
      "loss_all(train_data, model) = 0.16705358514225951\n",
      "loss_all(train_data, model) = 0.16754772797303175\n",
      "loss_all(train_data, model) = 0.1685708318530593\n",
      "loss_all(train_data, model) = 0.16990705248529253\n",
      "loss_all(train_data, model) = 0.1702599240993443\n",
      "loss_all(train_data, model) = 0.17107454462399688\n",
      "loss_all(train_data, model) = 0.17090331071836695\n",
      "loss_all(train_data, model) = 0.17070416084863796\n",
      "loss_all(train_data, model) = 0.170181325870823\n",
      "loss_all(train_data, model) = 0.16962831844821055\n",
      "loss_all(train_data, model) = 0.16787521889701995\n",
      "loss_all(train_data, model) = 0.16642421901858426\n",
      "loss_all(train_data, model) = 0.16609415690417023\n",
      "loss_all(train_data, model) = 0.16645296253106479\n",
      "loss_all(train_data, model) = 0.16779122118483938\n",
      "loss_all(train_data, model) = 0.17069826252560735\n",
      "loss_all(train_data, model) = 0.17364251033055783\n",
      "loss_all(train_data, model) = 0.17469523643548188\n",
      "loss_all(train_data, model) = 0.17495733729368074\n",
      "loss_all(train_data, model) = 0.1756745608778129\n",
      "loss_all(train_data, model) = 0.17723965302720157\n",
      "loss_all(train_data, model) = 0.17732919157518448\n",
      "loss_all(train_data, model) = 0.17565249768778426\n",
      "loss_all(train_data, model) = 0.17293349316788095\n",
      "loss_all(train_data, model) = 0.1694970091138814\n",
      "loss_all(train_data, model) = 0.16698085049237085\n",
      "loss_all(train_data, model) = 0.16575361772263467\n",
      "loss_all(train_data, model) = 0.16563580161632027\n",
      "loss_all(train_data, model) = 0.16633429216208287\n",
      "loss_all(train_data, model) = 0.16658087287656306\n",
      "loss_all(train_data, model) = 0.1673747972536333\n",
      "loss_all(train_data, model) = 0.1676642177275152\n",
      "loss_all(train_data, model) = 0.16819935627115118\n",
      "loss_all(train_data, model) = 0.16783225935478835\n",
      "loss_all(train_data, model) = 0.16687408433333334\n",
      "loss_all(train_data, model) = 0.1662753906775739\n",
      "loss_all(train_data, model) = 0.16581208922199073\n",
      "loss_all(train_data, model) = 0.16539899866789004\n",
      "loss_all(train_data, model) = 0.16523115015885834\n",
      "loss_all(train_data, model) = 0.16549673384094893\n",
      "loss_all(train_data, model) = 0.1659777184421887\n",
      "loss_all(train_data, model) = 0.16594642639111296\n",
      "loss_all(train_data, model) = 0.16588350712529212\n",
      "loss_all(train_data, model) = 0.16567612347071242\n",
      "loss_all(train_data, model) = 0.16562543870027355\n",
      "loss_all(train_data, model) = 0.16581027966674783\n",
      "loss_all(train_data, model) = 0.16577186902680954\n",
      "loss_all(train_data, model) = 0.16591023692066734\n",
      "loss_all(train_data, model) = 0.16586506528364925\n",
      "loss_all(train_data, model) = 0.1658453966738709\n",
      "loss_all(train_data, model) = 0.16527868197749282\n",
      "loss_all(train_data, model) = 0.16553650399548844\n",
      "loss_all(train_data, model) = 0.16685982913220682\n",
      "loss_all(train_data, model) = 0.16956912786331169\n",
      "loss_all(train_data, model) = 0.17257752338200172\n",
      "loss_all(train_data, model) = 0.1751876623699965\n",
      "loss_all(train_data, model) = 0.17624269835582507\n",
      "loss_all(train_data, model) = 0.17585225907045438\n",
      "loss_all(train_data, model) = 0.17373412461187368\n",
      "loss_all(train_data, model) = 0.1719424958075333\n",
      "loss_all(train_data, model) = 0.1702814813791418\n",
      "loss_all(train_data, model) = 0.16963116861147953\n",
      "loss_all(train_data, model) = 0.16834382684140067\n",
      "loss_all(train_data, model) = 0.1675081960023738\n",
      "loss_all(train_data, model) = 0.16721704855271816\n",
      "loss_all(train_data, model) = 0.16712729267555954\n",
      "loss_all(train_data, model) = 0.1671309245319933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.16758119136604877\n",
      "loss_all(train_data, model) = 0.16816431759790895\n",
      "loss_all(train_data, model) = 0.16880620840709917\n",
      "loss_all(train_data, model) = 0.16962064341608832\n",
      "loss_all(train_data, model) = 0.17052932403093643\n",
      "loss_all(train_data, model) = 0.17117829251086286\n",
      "loss_all(train_data, model) = 0.17217333032661955\n",
      "loss_all(train_data, model) = 0.17311521805586888\n",
      "loss_all(train_data, model) = 0.17454174633739847\n",
      "loss_all(train_data, model) = 0.17564984534252445\n",
      "loss_all(train_data, model) = 0.17369077547001985\n",
      "loss_all(train_data, model) = 0.1705010563886681\n",
      "loss_all(train_data, model) = 0.16840942098687467\n",
      "loss_all(train_data, model) = 0.16755363196568512\n",
      "loss_all(train_data, model) = 0.16738139610629088\n",
      "loss_all(train_data, model) = 0.16735177499186393\n",
      "loss_all(train_data, model) = 0.16733090199100997\n",
      "loss_all(train_data, model) = 0.16727663703952625\n",
      "loss_all(train_data, model) = 0.1670617030802385\n",
      "loss_all(train_data, model) = 0.16643161908647577\n",
      "loss_all(train_data, model) = 0.16604645118585512\n",
      "loss_all(train_data, model) = 0.166093579973827\n",
      "loss_all(train_data, model) = 0.16648928663558116\n",
      "loss_all(train_data, model) = 0.16703424554843957\n",
      "loss_all(train_data, model) = 0.1684190983268362\n",
      "loss_all(train_data, model) = 0.17033242488171377\n",
      "loss_all(train_data, model) = 0.17243103962026993\n",
      "loss_all(train_data, model) = 0.17308769738839666\n",
      "loss_all(train_data, model) = 0.17431049521135622\n",
      "loss_all(train_data, model) = 0.17634271508077207\n",
      "loss_all(train_data, model) = 0.17709813938297383\n",
      "loss_all(train_data, model) = 0.17600205874718552\n",
      "loss_all(train_data, model) = 0.17412736159336104\n",
      "loss_all(train_data, model) = 0.17021695172869278\n",
      "loss_all(train_data, model) = 0.1675824717852562\n",
      "loss_all(train_data, model) = 0.16644324232109137\n",
      "loss_all(train_data, model) = 0.16537969983216008\n",
      "loss_all(train_data, model) = 0.16494676252231383\n",
      "loss_all(train_data, model) = 0.1650044227824068\n",
      "loss_all(train_data, model) = 0.16537347584798687\n",
      "loss_all(train_data, model) = 0.1670108928543269\n",
      "loss_all(train_data, model) = 0.16845417912834554\n",
      "loss_all(train_data, model) = 0.1688924894732404\n",
      "loss_all(train_data, model) = 0.17020953726054416\n",
      "loss_all(train_data, model) = 0.17288838615167898\n",
      "loss_all(train_data, model) = 0.17559477470132476\n",
      "loss_all(train_data, model) = 0.1802934920022238\n",
      "loss_all(train_data, model) = 0.18473925955910217\n",
      "loss_all(train_data, model) = 0.1856747335142712\n",
      "loss_all(train_data, model) = 0.1831612165590659\n",
      "loss_all(train_data, model) = 0.17958669855511816\n",
      "loss_all(train_data, model) = 0.17646129412741474\n",
      "loss_all(train_data, model) = 0.17072932549501307\n",
      "loss_all(train_data, model) = 0.1669144890568603\n",
      "loss_all(train_data, model) = 0.1647730476143012\n",
      "loss_all(train_data, model) = 0.16709961635102488\n",
      "loss_all(train_data, model) = 0.17253034868020242\n",
      "loss_all(train_data, model) = 0.18028021795232557\n",
      "loss_all(train_data, model) = 0.18604433579029284\n",
      "loss_all(train_data, model) = 0.1926093972989738\n",
      "loss_all(train_data, model) = 0.1985512142398706\n",
      "loss_all(train_data, model) = 0.20663142348717334\n",
      "loss_all(train_data, model) = 0.20746114388856599\n",
      "loss_all(train_data, model) = 0.20600515558348712\n",
      "loss_all(train_data, model) = 0.20167425412244133\n",
      "loss_all(train_data, model) = 0.19295789925844184\n",
      "loss_all(train_data, model) = 0.18426416245901736\n",
      "loss_all(train_data, model) = 0.17626043237386496\n",
      "loss_all(train_data, model) = 0.17020203011559215\n",
      "loss_all(train_data, model) = 0.16747273629836187\n",
      "loss_all(train_data, model) = 0.16665057578028547\n",
      "loss_all(train_data, model) = 0.16647659069091958\n",
      "loss_all(train_data, model) = 0.16659145063073957\n",
      "loss_all(train_data, model) = 0.16691912256800512\n",
      "loss_all(train_data, model) = 0.16659641590201624\n",
      "loss_all(train_data, model) = 0.16635302744570774\n",
      "loss_all(train_data, model) = 0.16611080941371487\n",
      "loss_all(train_data, model) = 0.16586266302287284\n",
      "loss_all(train_data, model) = 0.16569935208711226\n",
      "loss_all(train_data, model) = 0.16574514524100292\n",
      "loss_all(train_data, model) = 0.16596624487084333\n",
      "loss_all(train_data, model) = 0.16716611441724608\n",
      "loss_all(train_data, model) = 0.16868376068177848\n",
      "loss_all(train_data, model) = 0.17036767499673533\n",
      "loss_all(train_data, model) = 0.1706639315909523\n",
      "loss_all(train_data, model) = 0.17139491705045556\n",
      "loss_all(train_data, model) = 0.17061895254288564\n",
      "loss_all(train_data, model) = 0.16951953272744041\n",
      "loss_all(train_data, model) = 0.16921899232577495\n",
      "loss_all(train_data, model) = 0.167842200110782\n",
      "loss_all(train_data, model) = 0.16593898056620837\n",
      "loss_all(train_data, model) = 0.16528060149890747\n",
      "loss_all(train_data, model) = 0.16519643823644273\n",
      "loss_all(train_data, model) = 0.16508015881024216\n",
      "loss_all(train_data, model) = 0.16519225071477223\n",
      "loss_all(train_data, model) = 0.16569633728452576\n",
      "loss_all(train_data, model) = 0.1668538620592945\n",
      "loss_all(train_data, model) = 0.16776115733770958\n",
      "loss_all(train_data, model) = 0.1687839245916594\n",
      "loss_all(train_data, model) = 0.16927711175781493\n",
      "loss_all(train_data, model) = 0.1683001143957695\n",
      "loss_all(train_data, model) = 0.16799236886407498\n",
      "loss_all(train_data, model) = 0.16769540817299544\n",
      "loss_all(train_data, model) = 0.1671827388133024\n",
      "loss_all(train_data, model) = 0.16710075735264596\n",
      "loss_all(train_data, model) = 0.1672798801005177\n",
      "loss_all(train_data, model) = 0.16745525309133064\n",
      "loss_all(train_data, model) = 0.1671839920371037\n",
      "loss_all(train_data, model) = 0.16799017781988718\n",
      "loss_all(train_data, model) = 0.1697445454332289\n",
      "loss_all(train_data, model) = 0.17261560260581488\n",
      "loss_all(train_data, model) = 0.17769840144329627\n",
      "loss_all(train_data, model) = 0.18343910574977204\n",
      "loss_all(train_data, model) = 0.18860614263826714\n",
      "loss_all(train_data, model) = 0.1952259558940526\n",
      "loss_all(train_data, model) = 0.20325098271531455\n",
      "loss_all(train_data, model) = 0.20699298055473392\n",
      "loss_all(train_data, model) = 0.2074011022426544\n",
      "loss_all(train_data, model) = 0.20292931403413797\n",
      "loss_all(train_data, model) = 0.1967914654464082\n",
      "loss_all(train_data, model) = 0.18626623497171668\n",
      "loss_all(train_data, model) = 0.1749397839727303\n",
      "loss_all(train_data, model) = 0.16884201130667403\n",
      "loss_all(train_data, model) = 0.16540743497267765\n",
      "loss_all(train_data, model) = 0.16619413191688048\n",
      "loss_all(train_data, model) = 0.16982437572528403\n",
      "loss_all(train_data, model) = 0.1769256397930329\n",
      "loss_all(train_data, model) = 0.18668460996554334\n",
      "loss_all(train_data, model) = 0.19829206889834708\n",
      "loss_all(train_data, model) = 0.20041377389759737\n",
      "loss_all(train_data, model) = 0.2003223986494578\n",
      "loss_all(train_data, model) = 0.1965582982023721\n",
      "loss_all(train_data, model) = 0.19055951167954482\n",
      "loss_all(train_data, model) = 0.18403474249141138\n",
      "loss_all(train_data, model) = 0.17774513119301624\n",
      "loss_all(train_data, model) = 0.17194707858854882\n",
      "loss_all(train_data, model) = 0.16893752327535752\n",
      "loss_all(train_data, model) = 0.16713557345680177\n",
      "loss_all(train_data, model) = 0.1657891074818643\n",
      "loss_all(train_data, model) = 0.16522622874719328\n",
      "loss_all(train_data, model) = 0.1651083258935346\n",
      "loss_all(train_data, model) = 0.16624599446808241\n",
      "loss_all(train_data, model) = 0.1670727314337656\n",
      "loss_all(train_data, model) = 0.1681475991214618\n",
      "loss_all(train_data, model) = 0.16893832903870973\n",
      "loss_all(train_data, model) = 0.170670257411908\n",
      "loss_all(train_data, model) = 0.17221059151995177\n",
      "loss_all(train_data, model) = 0.1729777313185835\n",
      "loss_all(train_data, model) = 0.17295072005168988\n",
      "loss_all(train_data, model) = 0.17161183400474275\n",
      "loss_all(train_data, model) = 0.16943016619207568\n",
      "loss_all(train_data, model) = 0.1663026304725448\n",
      "loss_all(train_data, model) = 0.16517991733990006\n",
      "loss_all(train_data, model) = 0.16443918120221476\n",
      "loss_all(train_data, model) = 0.1645597551515182\n",
      "loss_all(train_data, model) = 0.16691577855322817\n",
      "loss_all(train_data, model) = 0.17117720263341626\n",
      "loss_all(train_data, model) = 0.17675099761631585\n",
      "loss_all(train_data, model) = 0.18119249541883245\n",
      "loss_all(train_data, model) = 0.18055830559306385\n",
      "loss_all(train_data, model) = 0.17724579941001153\n",
      "loss_all(train_data, model) = 0.1753954602119845\n",
      "loss_all(train_data, model) = 0.17177019684614822\n",
      "loss_all(train_data, model) = 0.16882692300983865\n",
      "loss_all(train_data, model) = 0.1667219459639037\n",
      "loss_all(train_data, model) = 0.16522641501354693\n",
      "loss_all(train_data, model) = 0.16464249531637057\n",
      "loss_all(train_data, model) = 0.16462985770772673\n",
      "loss_all(train_data, model) = 0.1649335470293363\n",
      "loss_all(train_data, model) = 0.16549795773924178\n",
      "loss_all(train_data, model) = 0.1667914791349856\n",
      "loss_all(train_data, model) = 0.16813291024784513\n",
      "loss_all(train_data, model) = 0.16789384104760544\n",
      "loss_all(train_data, model) = 0.16684645179625826\n",
      "loss_all(train_data, model) = 0.16531541334125974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.16495175597738146\n",
      "loss_all(train_data, model) = 0.1658774563940207\n",
      "loss_all(train_data, model) = 0.1683563768031609\n",
      "loss_all(train_data, model) = 0.17001321338716302\n",
      "loss_all(train_data, model) = 0.17372673650304957\n",
      "loss_all(train_data, model) = 0.17777387849510257\n",
      "loss_all(train_data, model) = 0.18171071676078113\n",
      "loss_all(train_data, model) = 0.18058476477103505\n",
      "loss_all(train_data, model) = 0.17891681410534943\n",
      "loss_all(train_data, model) = 0.1742656826041038\n",
      "loss_all(train_data, model) = 0.17216824149096455\n",
      "loss_all(train_data, model) = 0.17252246646084307\n",
      "loss_all(train_data, model) = 0.1718012664080308\n",
      "loss_all(train_data, model) = 0.17037468438063735\n",
      "loss_all(train_data, model) = 0.1692430807504968\n",
      "loss_all(train_data, model) = 0.16694723054349184\n",
      "loss_all(train_data, model) = 0.1657231372800754\n",
      "loss_all(train_data, model) = 0.16551017185197864\n",
      "loss_all(train_data, model) = 0.16601747489025645\n",
      "loss_all(train_data, model) = 0.16630305363603246\n",
      "loss_all(train_data, model) = 0.16617852487634366\n",
      "loss_all(train_data, model) = 0.16627807154117052\n",
      "loss_all(train_data, model) = 0.16578930297029382\n",
      "loss_all(train_data, model) = 0.1654071457745946\n",
      "loss_all(train_data, model) = 0.16540327320960918\n",
      "loss_all(train_data, model) = 0.16544360339904235\n",
      "loss_all(train_data, model) = 0.16563573310690635\n",
      "loss_all(train_data, model) = 0.166245557528702\n",
      "loss_all(train_data, model) = 0.16630713049065213\n",
      "loss_all(train_data, model) = 0.1666736479453563\n",
      "loss_all(train_data, model) = 0.16705760628723187\n",
      "loss_all(train_data, model) = 0.16729621295247107\n",
      "loss_all(train_data, model) = 0.16677290362091074\n",
      "loss_all(train_data, model) = 0.1661185140059668\n",
      "loss_all(train_data, model) = 0.16598621096741442\n",
      "loss_all(train_data, model) = 0.16619063092698036\n",
      "loss_all(train_data, model) = 0.16583785823273126\n",
      "loss_all(train_data, model) = 0.16570821172403008\n",
      "loss_all(train_data, model) = 0.16648927784494866\n",
      "loss_all(train_data, model) = 0.1667443866428603\n",
      "loss_all(train_data, model) = 0.16664772564300037\n",
      "loss_all(train_data, model) = 0.16603767912590245\n",
      "loss_all(train_data, model) = 0.1664095280529722\n",
      "loss_all(train_data, model) = 0.16720540403364606\n",
      "loss_all(train_data, model) = 0.16759743366033086\n",
      "loss_all(train_data, model) = 0.16807187803427387\n",
      "loss_all(train_data, model) = 0.16835055563686066\n",
      "loss_all(train_data, model) = 0.16746078365687378\n",
      "loss_all(train_data, model) = 0.1668063390884844\n",
      "loss_all(train_data, model) = 0.1663754018973383\n",
      "loss_all(train_data, model) = 0.16625569468478493\n",
      "loss_all(train_data, model) = 0.16672441086160386\n",
      "loss_all(train_data, model) = 0.16768559745458073\n",
      "loss_all(train_data, model) = 0.1680219763390693\n",
      "loss_all(train_data, model) = 0.1692947360721205\n",
      "loss_all(train_data, model) = 0.16931678010465329\n",
      "loss_all(train_data, model) = 0.16906152832991422\n",
      "loss_all(train_data, model) = 0.16837073990290444\n",
      "loss_all(train_data, model) = 0.16829801720632298\n",
      "loss_all(train_data, model) = 0.1675567876925788\n",
      "loss_all(train_data, model) = 0.16674988460964627\n",
      "loss_all(train_data, model) = 0.16614760157690503\n",
      "loss_all(train_data, model) = 0.1663930730945303\n",
      "loss_all(train_data, model) = 0.1663014380341888\n",
      "loss_all(train_data, model) = 0.16641113820396736\n",
      "loss_all(train_data, model) = 0.16594764269286777\n",
      "loss_all(train_data, model) = 0.16606712149003114\n",
      "loss_all(train_data, model) = 0.16603390158037085\n",
      "loss_all(train_data, model) = 0.16613067854765198\n",
      "loss_all(train_data, model) = 0.16522563628722886\n",
      "loss_all(train_data, model) = 0.16473455074175591\n",
      "loss_all(train_data, model) = 0.16473904725287836\n",
      "loss_all(train_data, model) = 0.1655471664066556\n",
      "loss_all(train_data, model) = 0.16635172651127228\n",
      "loss_all(train_data, model) = 0.16672111917414836\n",
      "loss_all(train_data, model) = 0.16702732983301732\n",
      "loss_all(train_data, model) = 0.16816278171193114\n",
      "loss_all(train_data, model) = 0.16853073153150927\n",
      "loss_all(train_data, model) = 0.16851191284863298\n",
      "loss_all(train_data, model) = 0.16828029690018598\n",
      "loss_all(train_data, model) = 0.16744519399511928\n",
      "loss_all(train_data, model) = 0.16655727205684168\n",
      "loss_all(train_data, model) = 0.16591189388522715\n",
      "loss_all(train_data, model) = 0.16573588316587953\n",
      "loss_all(train_data, model) = 0.16692963540938216\n",
      "loss_all(train_data, model) = 0.16919443216592311\n",
      "loss_all(train_data, model) = 0.17141515177149053\n",
      "loss_all(train_data, model) = 0.172516702879445\n",
      "loss_all(train_data, model) = 0.17311637849141454\n",
      "loss_all(train_data, model) = 0.1734485920626132\n",
      "loss_all(train_data, model) = 0.17372801034691862\n",
      "loss_all(train_data, model) = 0.17190511798503066\n",
      "loss_all(train_data, model) = 0.16808284339249036\n",
      "loss_all(train_data, model) = 0.16634715605593725\n",
      "loss_all(train_data, model) = 0.16546942869503897\n",
      "loss_all(train_data, model) = 0.16572060506463157\n",
      "loss_all(train_data, model) = 0.1667669745102308\n",
      "loss_all(train_data, model) = 0.16779958773172082\n",
      "loss_all(train_data, model) = 0.1689850527711673\n",
      "loss_all(train_data, model) = 0.16860819839423988\n",
      "loss_all(train_data, model) = 0.16755072133341192\n",
      "loss_all(train_data, model) = 0.1662426431957678\n",
      "loss_all(train_data, model) = 0.165402069687407\n",
      "loss_all(train_data, model) = 0.1648569228060919\n",
      "loss_all(train_data, model) = 0.16477189263852546\n",
      "loss_all(train_data, model) = 0.1648207160549317\n",
      "loss_all(train_data, model) = 0.1647931288004352\n",
      "loss_all(train_data, model) = 0.16477791029236669\n",
      "loss_all(train_data, model) = 0.16503112029415448\n",
      "loss_all(train_data, model) = 0.16516441580171443\n",
      "loss_all(train_data, model) = 0.16587173563127808\n",
      "loss_all(train_data, model) = 0.16587532650665654\n",
      "loss_all(train_data, model) = 0.16560472011937472\n",
      "loss_all(train_data, model) = 0.16530412600261044\n",
      "loss_all(train_data, model) = 0.16507641805657694\n",
      "loss_all(train_data, model) = 0.1649556744933702\n",
      "loss_all(train_data, model) = 0.16516131493934946\n",
      "loss_all(train_data, model) = 0.16574741221885786\n",
      "loss_all(train_data, model) = 0.16646973991722328\n",
      "loss_all(train_data, model) = 0.1665613671933106\n",
      "loss_all(train_data, model) = 0.16654883469392465\n",
      "loss_all(train_data, model) = 0.16611301824750482\n",
      "loss_all(train_data, model) = 0.1658652664986576\n",
      "loss_all(train_data, model) = 0.16539475010829738\n",
      "loss_all(train_data, model) = 0.1652505894145507\n",
      "loss_all(train_data, model) = 0.165217440720917\n",
      "loss_all(train_data, model) = 0.1656200462651009\n",
      "loss_all(train_data, model) = 0.1658300512726021\n",
      "loss_all(train_data, model) = 0.16564794582202097\n",
      "loss_all(train_data, model) = 0.16542102268263315\n",
      "loss_all(train_data, model) = 0.16520385100705615\n",
      "loss_all(train_data, model) = 0.1652283759274946\n",
      "loss_all(train_data, model) = 0.16523938374636846\n",
      "loss_all(train_data, model) = 0.16521980302062106\n",
      "loss_all(train_data, model) = 0.16547762948163314\n",
      "loss_all(train_data, model) = 0.16559023190079647\n",
      "loss_all(train_data, model) = 0.16585931926900727\n",
      "loss_all(train_data, model) = 0.16673926419512036\n",
      "loss_all(train_data, model) = 0.16748973951568275\n",
      "loss_all(train_data, model) = 0.16836488519207343\n",
      "loss_all(train_data, model) = 0.1689856014653217\n",
      "loss_all(train_data, model) = 0.16875454457287925\n",
      "loss_all(train_data, model) = 0.16838298106579933\n",
      "loss_all(train_data, model) = 0.16707433698529964\n",
      "loss_all(train_data, model) = 0.16604935441414867\n",
      "loss_all(train_data, model) = 0.167138808193621\n",
      "loss_all(train_data, model) = 0.1699839649623628\n",
      "loss_all(train_data, model) = 0.17373553516424875\n",
      "loss_all(train_data, model) = 0.17766197837530295\n",
      "loss_all(train_data, model) = 0.17993543733039463\n",
      "loss_all(train_data, model) = 0.18512650405834438\n",
      "loss_all(train_data, model) = 0.1886587184444179\n",
      "loss_all(train_data, model) = 0.19018650530171807\n",
      "loss_all(train_data, model) = 0.19314306475808088\n",
      "loss_all(train_data, model) = 0.1886672179502324\n",
      "loss_all(train_data, model) = 0.18317559233827718\n",
      "loss_all(train_data, model) = 0.17774863416515188\n",
      "loss_all(train_data, model) = 0.17389973384106353\n",
      "loss_all(train_data, model) = 0.17144378858739626\n",
      "loss_all(train_data, model) = 0.17000307429051506\n",
      "loss_all(train_data, model) = 0.16842662090065347\n",
      "loss_all(train_data, model) = 0.16892393984814205\n",
      "loss_all(train_data, model) = 0.17233157634404064\n",
      "loss_all(train_data, model) = 0.1798867572747139\n",
      "loss_all(train_data, model) = 0.18806778581949976\n",
      "loss_all(train_data, model) = 0.19882954455149315\n",
      "loss_all(train_data, model) = 0.20716151629032356\n",
      "loss_all(train_data, model) = 0.21222003606952972\n",
      "loss_all(train_data, model) = 0.21322905142530904\n",
      "loss_all(train_data, model) = 0.21049900293071191\n",
      "loss_all(train_data, model) = 0.20637295327325347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.19909735327771114\n",
      "loss_all(train_data, model) = 0.19575075189484964\n",
      "loss_all(train_data, model) = 0.1909600557516814\n",
      "loss_all(train_data, model) = 0.18171284328561074\n",
      "loss_all(train_data, model) = 0.17501944804221808\n",
      "loss_all(train_data, model) = 0.1715747708199098\n",
      "loss_all(train_data, model) = 0.1723193148197102\n",
      "loss_all(train_data, model) = 0.17832325026000737\n",
      "loss_all(train_data, model) = 0.18941173212120943\n",
      "loss_all(train_data, model) = 0.19702782886732428\n",
      "loss_all(train_data, model) = 0.19802467532584636\n",
      "loss_all(train_data, model) = 0.19455009040151297\n",
      "loss_all(train_data, model) = 0.19229830315643265\n",
      "loss_all(train_data, model) = 0.19026557478448264\n",
      "loss_all(train_data, model) = 0.18610049732168651\n",
      "loss_all(train_data, model) = 0.18189465787100004\n",
      "loss_all(train_data, model) = 0.177130314302832\n",
      "loss_all(train_data, model) = 0.17284398205947032\n",
      "loss_all(train_data, model) = 0.17056958711891623\n",
      "loss_all(train_data, model) = 0.17042817201179994\n",
      "loss_all(train_data, model) = 0.17260013289279705\n",
      "loss_all(train_data, model) = 0.17639876050601194\n",
      "loss_all(train_data, model) = 0.18095954536769235\n",
      "loss_all(train_data, model) = 0.18722465194284627\n",
      "loss_all(train_data, model) = 0.19099483377036833\n",
      "loss_all(train_data, model) = 0.19409451016312843\n",
      "loss_all(train_data, model) = 0.19515724762485345\n",
      "loss_all(train_data, model) = 0.19541851885649683\n",
      "loss_all(train_data, model) = 0.19466896439764658\n",
      "loss_all(train_data, model) = 0.190245306496115\n",
      "loss_all(train_data, model) = 0.18583188850091162\n",
      "loss_all(train_data, model) = 0.1810665722028052\n",
      "loss_all(train_data, model) = 0.17862793148090886\n",
      "loss_all(train_data, model) = 0.1738130633586872\n",
      "loss_all(train_data, model) = 0.17044202296332775\n",
      "loss_all(train_data, model) = 0.1686991728907569\n",
      "loss_all(train_data, model) = 0.16853086789602142\n",
      "loss_all(train_data, model) = 0.16998167613014786\n",
      "loss_all(train_data, model) = 0.17131432338133498\n",
      "loss_all(train_data, model) = 0.17556354830581208\n",
      "loss_all(train_data, model) = 0.17822749990116143\n",
      "loss_all(train_data, model) = 0.18235268030070748\n",
      "loss_all(train_data, model) = 0.18533646445752341\n",
      "loss_all(train_data, model) = 0.18452476603418083\n",
      "loss_all(train_data, model) = 0.18208548705913738\n",
      "loss_all(train_data, model) = 0.17930747607648967\n",
      "loss_all(train_data, model) = 0.17529708812809353\n",
      "loss_all(train_data, model) = 0.17283064081762126\n",
      "loss_all(train_data, model) = 0.16993679013330387\n",
      "loss_all(train_data, model) = 0.16860873848758337\n",
      "loss_all(train_data, model) = 0.16774129111788874\n",
      "loss_all(train_data, model) = 0.16687621075315417\n",
      "loss_all(train_data, model) = 0.16640867734141732\n",
      "loss_all(train_data, model) = 0.16667173599633697\n",
      "loss_all(train_data, model) = 0.16734879544672304\n",
      "loss_all(train_data, model) = 0.1678108832259437\n",
      "loss_all(train_data, model) = 0.16876225744733597\n",
      "loss_all(train_data, model) = 0.1687615549530222\n",
      "loss_all(train_data, model) = 0.16707441737695874\n",
      "loss_all(train_data, model) = 0.16600473094394325\n",
      "loss_all(train_data, model) = 0.1655533381582003\n",
      "loss_all(train_data, model) = 0.16551278840796954\n",
      "loss_all(train_data, model) = 0.1652749717392416\n",
      "loss_all(train_data, model) = 0.1652423365370661\n",
      "loss_all(train_data, model) = 0.16605811573933232\n",
      "loss_all(train_data, model) = 0.16804687135591778\n",
      "loss_all(train_data, model) = 0.17015483947213117\n",
      "loss_all(train_data, model) = 0.17287109986622934\n",
      "loss_all(train_data, model) = 0.17554823865333327\n",
      "loss_all(train_data, model) = 0.17964937590176824\n",
      "loss_all(train_data, model) = 0.18372299082729768\n",
      "loss_all(train_data, model) = 0.18911129447307723\n",
      "loss_all(train_data, model) = 0.19377740749548078\n",
      "loss_all(train_data, model) = 0.19822855597299163\n",
      "loss_all(train_data, model) = 0.2010421631507529\n",
      "loss_all(train_data, model) = 0.20374498154040807\n",
      "loss_all(train_data, model) = 0.2045531667844792\n",
      "loss_all(train_data, model) = 0.1998492058149854\n",
      "loss_all(train_data, model) = 0.19046031387866807\n",
      "loss_all(train_data, model) = 0.17937169591309604\n",
      "loss_all(train_data, model) = 0.17277549967906705\n",
      "loss_all(train_data, model) = 0.16734382676191442\n",
      "loss_all(train_data, model) = 0.1647826317513007\n",
      "loss_all(train_data, model) = 0.16494896585501911\n",
      "loss_all(train_data, model) = 0.16704704433261483\n",
      "loss_all(train_data, model) = 0.16867044950037427\n",
      "loss_all(train_data, model) = 0.1705522073973943\n",
      "loss_all(train_data, model) = 0.17210670032361616\n",
      "loss_all(train_data, model) = 0.17297988121724633\n",
      "loss_all(train_data, model) = 0.17057527066257197\n",
      "loss_all(train_data, model) = 0.16823051113907483\n",
      "loss_all(train_data, model) = 0.16606274321547704\n",
      "loss_all(train_data, model) = 0.16510551123387554\n",
      "loss_all(train_data, model) = 0.1649150374291111\n",
      "loss_all(train_data, model) = 0.16557991531899904\n",
      "loss_all(train_data, model) = 0.16832913300794322\n",
      "loss_all(train_data, model) = 0.17007569609627834\n",
      "loss_all(train_data, model) = 0.17197745948638588\n",
      "loss_all(train_data, model) = 0.17361031211286687\n",
      "loss_all(train_data, model) = 0.17612085720188217\n",
      "loss_all(train_data, model) = 0.1770656593193655\n",
      "loss_all(train_data, model) = 0.17618350503017197\n",
      "loss_all(train_data, model) = 0.17455171527886562\n",
      "loss_all(train_data, model) = 0.17208238599242323\n",
      "loss_all(train_data, model) = 0.16919157626066247\n",
      "loss_all(train_data, model) = 0.16694407399862457\n",
      "loss_all(train_data, model) = 0.16555092016916811\n",
      "loss_all(train_data, model) = 0.16485154649423026\n",
      "loss_all(train_data, model) = 0.1646835516196629\n",
      "loss_all(train_data, model) = 0.16538099448648824\n",
      "loss_all(train_data, model) = 0.16690417673266267\n",
      "loss_all(train_data, model) = 0.1679210421074239\n",
      "loss_all(train_data, model) = 0.1685225535616007\n",
      "loss_all(train_data, model) = 0.167154724984321\n",
      "loss_all(train_data, model) = 0.1659689071806419\n",
      "loss_all(train_data, model) = 0.16551116622502313\n",
      "loss_all(train_data, model) = 0.16566542472781584\n",
      "loss_all(train_data, model) = 0.16570591308053587\n",
      "loss_all(train_data, model) = 0.16567158003439278\n",
      "loss_all(train_data, model) = 0.16583488424599152\n",
      "loss_all(train_data, model) = 0.16603938573768529\n",
      "loss_all(train_data, model) = 0.166577713297916\n",
      "loss_all(train_data, model) = 0.16727396801896152\n",
      "loss_all(train_data, model) = 0.16859109680703696\n",
      "loss_all(train_data, model) = 0.16906373577583525\n",
      "loss_all(train_data, model) = 0.16913676839061093\n",
      "loss_all(train_data, model) = 0.16806150133636727\n",
      "loss_all(train_data, model) = 0.16638414318606215\n",
      "loss_all(train_data, model) = 0.16563250744543595\n",
      "loss_all(train_data, model) = 0.16539211524790268\n",
      "loss_all(train_data, model) = 0.16534224755491495\n",
      "loss_all(train_data, model) = 0.16548673991204368\n",
      "loss_all(train_data, model) = 0.165725677229248\n",
      "loss_all(train_data, model) = 0.1659489267449413\n",
      "loss_all(train_data, model) = 0.16717908568876966\n",
      "loss_all(train_data, model) = 0.16848926818904877\n",
      "loss_all(train_data, model) = 0.16925915246780177\n",
      "loss_all(train_data, model) = 0.16998975244450765\n",
      "loss_all(train_data, model) = 0.16987246184360696\n",
      "loss_all(train_data, model) = 0.16888086376419634\n",
      "loss_all(train_data, model) = 0.16713640357421714\n",
      "loss_all(train_data, model) = 0.16629566852309396\n",
      "loss_all(train_data, model) = 0.16570298314765874\n",
      "loss_all(train_data, model) = 0.1653692399294367\n",
      "loss_all(train_data, model) = 0.1653925813943786\n",
      "loss_all(train_data, model) = 0.16549770625495785\n",
      "loss_all(train_data, model) = 0.16559476783428742\n",
      "loss_all(train_data, model) = 0.16561289540670554\n",
      "loss_all(train_data, model) = 0.1656838073279873\n",
      "loss_all(train_data, model) = 0.16587916340294687\n",
      "loss_all(train_data, model) = 0.16648645555151007\n",
      "loss_all(train_data, model) = 0.1689695914124151\n",
      "loss_all(train_data, model) = 0.17018513290206883\n",
      "loss_all(train_data, model) = 0.17289457125820865\n",
      "loss_all(train_data, model) = 0.1758303278514682\n",
      "loss_all(train_data, model) = 0.17934942709020055\n",
      "loss_all(train_data, model) = 0.1781580878964813\n",
      "loss_all(train_data, model) = 0.17690225222849362\n",
      "loss_all(train_data, model) = 0.1737038983938265\n",
      "loss_all(train_data, model) = 0.17114040003954353\n",
      "loss_all(train_data, model) = 0.1689838499818845\n",
      "loss_all(train_data, model) = 0.16649790410508708\n",
      "loss_all(train_data, model) = 0.1649048618488949\n",
      "loss_all(train_data, model) = 0.16453217668975784\n",
      "loss_all(train_data, model) = 0.16588406473437017\n",
      "loss_all(train_data, model) = 0.16996266113155678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.1755695497630673\n",
      "loss_all(train_data, model) = 0.18056744928294258\n",
      "loss_all(train_data, model) = 0.1877162352626174\n",
      "loss_all(train_data, model) = 0.19154416985926714\n",
      "loss_all(train_data, model) = 0.19643866734422497\n",
      "loss_all(train_data, model) = 0.19653664883804425\n",
      "loss_all(train_data, model) = 0.1951238485995924\n",
      "loss_all(train_data, model) = 0.19021434481958774\n",
      "loss_all(train_data, model) = 0.18422127334610952\n",
      "loss_all(train_data, model) = 0.17661966359830333\n",
      "loss_all(train_data, model) = 0.1718116028760915\n",
      "loss_all(train_data, model) = 0.16876919931454404\n",
      "loss_all(train_data, model) = 0.16660812833755273\n",
      "loss_all(train_data, model) = 0.1651661447138314\n",
      "loss_all(train_data, model) = 0.16474210430037117\n",
      "loss_all(train_data, model) = 0.16536349880786658\n",
      "loss_all(train_data, model) = 0.16817457736231767\n",
      "loss_all(train_data, model) = 0.17169751017752705\n",
      "loss_all(train_data, model) = 0.17488435415151612\n",
      "loss_all(train_data, model) = 0.17500646287982846\n",
      "loss_all(train_data, model) = 0.1731720281620408\n",
      "loss_all(train_data, model) = 0.17057822389097407\n",
      "loss_all(train_data, model) = 0.16920636059397917\n",
      "loss_all(train_data, model) = 0.16887036286601534\n",
      "loss_all(train_data, model) = 0.17061861633731962\n",
      "loss_all(train_data, model) = 0.17354389238529253\n",
      "loss_all(train_data, model) = 0.17471264350184287\n",
      "loss_all(train_data, model) = 0.17410506193735015\n",
      "loss_all(train_data, model) = 0.17428405995200769\n",
      "loss_all(train_data, model) = 0.17255256132004018\n",
      "loss_all(train_data, model) = 0.17280105198287043\n",
      "loss_all(train_data, model) = 0.17298056963056782\n",
      "loss_all(train_data, model) = 0.17139393347134105\n",
      "loss_all(train_data, model) = 0.16891004206736487\n",
      "loss_all(train_data, model) = 0.16683577415191547\n",
      "loss_all(train_data, model) = 0.16582644343060063\n",
      "loss_all(train_data, model) = 0.1652329694676336\n",
      "loss_all(train_data, model) = 0.16520540608150686\n",
      "loss_all(train_data, model) = 0.1657836023312388\n",
      "loss_all(train_data, model) = 0.1674636783165515\n",
      "loss_all(train_data, model) = 0.17006605859626125\n",
      "loss_all(train_data, model) = 0.17270910675874665\n",
      "loss_all(train_data, model) = 0.173388655109204\n",
      "loss_all(train_data, model) = 0.1730811837045177\n",
      "loss_all(train_data, model) = 0.17303276117221347\n",
      "loss_all(train_data, model) = 0.1736335235116807\n",
      "loss_all(train_data, model) = 0.1728728446977995\n",
      "loss_all(train_data, model) = 0.1729257997473911\n",
      "loss_all(train_data, model) = 0.17334022617370093\n",
      "loss_all(train_data, model) = 0.17298362649610857\n",
      "loss_all(train_data, model) = 0.17296955771183994\n",
      "loss_all(train_data, model) = 0.17271070450114218\n",
      "loss_all(train_data, model) = 0.17205028728309038\n",
      "loss_all(train_data, model) = 0.17221997377823317\n",
      "loss_all(train_data, model) = 0.17391435779795167\n",
      "loss_all(train_data, model) = 0.175500294520538\n",
      "loss_all(train_data, model) = 0.17722814226731112\n",
      "loss_all(train_data, model) = 0.17917004158370006\n",
      "loss_all(train_data, model) = 0.1836448204141381\n",
      "loss_all(train_data, model) = 0.18735179868194418\n",
      "loss_all(train_data, model) = 0.18754209576360376\n",
      "loss_all(train_data, model) = 0.18793263370524022\n",
      "loss_all(train_data, model) = 0.18607789751368656\n",
      "loss_all(train_data, model) = 0.18096339686914692\n",
      "loss_all(train_data, model) = 0.17798484033510026\n",
      "loss_all(train_data, model) = 0.17445143823601303\n",
      "loss_all(train_data, model) = 0.17059328388990877\n",
      "loss_all(train_data, model) = 0.16668153851682538\n",
      "loss_all(train_data, model) = 0.16464575065634318\n",
      "loss_all(train_data, model) = 0.16467428412403934\n",
      "loss_all(train_data, model) = 0.16666619585538456\n",
      "loss_all(train_data, model) = 0.1700831411554876\n",
      "loss_all(train_data, model) = 0.17603243664763357\n",
      "loss_all(train_data, model) = 0.17992643156058655\n",
      "loss_all(train_data, model) = 0.18386423133674598\n",
      "loss_all(train_data, model) = 0.18648716136578194\n",
      "loss_all(train_data, model) = 0.18518797458277228\n",
      "loss_all(train_data, model) = 0.18354973042771205\n",
      "loss_all(train_data, model) = 0.18086069537346355\n",
      "loss_all(train_data, model) = 0.17619242393969015\n",
      "loss_all(train_data, model) = 0.17236289402052074\n",
      "loss_all(train_data, model) = 0.16889470738489465\n",
      "loss_all(train_data, model) = 0.16643530401333903\n",
      "loss_all(train_data, model) = 0.1649985222946478\n",
      "loss_all(train_data, model) = 0.16483831457540168\n",
      "loss_all(train_data, model) = 0.16579462685182392\n",
      "loss_all(train_data, model) = 0.16919538262245923\n",
      "loss_all(train_data, model) = 0.1733253425381336\n",
      "loss_all(train_data, model) = 0.17735116631061623\n",
      "loss_all(train_data, model) = 0.178323288225275\n",
      "loss_all(train_data, model) = 0.17808856631222655\n",
      "loss_all(train_data, model) = 0.17295300511626116\n",
      "loss_all(train_data, model) = 0.16962536112354507\n",
      "loss_all(train_data, model) = 0.16740773655382601\n",
      "loss_all(train_data, model) = 0.16639847230698618\n",
      "loss_all(train_data, model) = 0.1666247587169404\n",
      "loss_all(train_data, model) = 0.16686806586718822\n",
      "loss_all(train_data, model) = 0.16664943058058085\n",
      "loss_all(train_data, model) = 0.1662052946465765\n",
      "loss_all(train_data, model) = 0.1658108654350999\n",
      "loss_all(train_data, model) = 0.16559458514995906\n",
      "loss_all(train_data, model) = 0.16543252254155785\n",
      "loss_all(train_data, model) = 0.16533248449320637\n",
      "loss_all(train_data, model) = 0.16524204577326235\n",
      "loss_all(train_data, model) = 0.16503243488448635\n",
      "loss_all(train_data, model) = 0.1650216948358505\n",
      "loss_all(train_data, model) = 0.16541686579360013\n",
      "loss_all(train_data, model) = 0.16594170782967704\n",
      "loss_all(train_data, model) = 0.166041795488392\n",
      "loss_all(train_data, model) = 0.1652998400258776\n",
      "loss_all(train_data, model) = 0.16497410946483143\n",
      "loss_all(train_data, model) = 0.1648007599692717\n",
      "loss_all(train_data, model) = 0.16478508443524287\n",
      "loss_all(train_data, model) = 0.16482456649525062\n",
      "loss_all(train_data, model) = 0.16477052281043758\n",
      "loss_all(train_data, model) = 0.16491741724954037\n",
      "loss_all(train_data, model) = 0.1648989674004357\n",
      "loss_all(train_data, model) = 0.16499486072664515\n",
      "loss_all(train_data, model) = 0.16510679358247612\n",
      "loss_all(train_data, model) = 0.16526980147143228\n",
      "loss_all(train_data, model) = 0.16534363561079365\n",
      "loss_all(train_data, model) = 0.16541100870417316\n",
      "loss_all(train_data, model) = 0.16552196237705027\n",
      "loss_all(train_data, model) = 0.16559052058031826\n",
      "loss_all(train_data, model) = 0.16572557560604698\n",
      "loss_all(train_data, model) = 0.1674352892881628\n",
      "loss_all(train_data, model) = 0.17053276907359521\n",
      "loss_all(train_data, model) = 0.17287752098454248\n",
      "loss_all(train_data, model) = 0.17510615541918315\n",
      "loss_all(train_data, model) = 0.17362632347286905\n",
      "loss_all(train_data, model) = 0.1702727086683867\n",
      "loss_all(train_data, model) = 0.16807591328006613\n",
      "loss_all(train_data, model) = 0.1663259132625985\n",
      "loss_all(train_data, model) = 0.16574941821555608\n",
      "loss_all(train_data, model) = 0.16563324350516404\n",
      "loss_all(train_data, model) = 0.16626149086484532\n",
      "loss_all(train_data, model) = 0.1680543156497101\n",
      "loss_all(train_data, model) = 0.17096305570331044\n",
      "loss_all(train_data, model) = 0.17459782524253212\n",
      "loss_all(train_data, model) = 0.1770491614574623\n",
      "loss_all(train_data, model) = 0.1832986889466891\n",
      "loss_all(train_data, model) = 0.18511189719582727\n",
      "loss_all(train_data, model) = 0.18725070306197394\n",
      "loss_all(train_data, model) = 0.1875077828775781\n",
      "loss_all(train_data, model) = 0.18510333875538057\n",
      "loss_all(train_data, model) = 0.1808234541307985\n",
      "loss_all(train_data, model) = 0.17583753292439466\n",
      "loss_all(train_data, model) = 0.17430980299710594\n",
      "loss_all(train_data, model) = 0.1729678735331476\n",
      "loss_all(train_data, model) = 0.17099123430815813\n",
      "loss_all(train_data, model) = 0.16979308994381304\n",
      "loss_all(train_data, model) = 0.16818626751740973\n",
      "loss_all(train_data, model) = 0.166462542698084\n",
      "loss_all(train_data, model) = 0.16523686023037032\n",
      "loss_all(train_data, model) = 0.16429622875047384\n",
      "loss_all(train_data, model) = 0.16421725257614211\n",
      "loss_all(train_data, model) = 0.16445042500624002\n",
      "loss_all(train_data, model) = 0.16455162990804353\n",
      "loss_all(train_data, model) = 0.16462914271343557\n",
      "loss_all(train_data, model) = 0.16534199801153765\n",
      "loss_all(train_data, model) = 0.16617966598990813\n",
      "loss_all(train_data, model) = 0.16729264158208115\n",
      "loss_all(train_data, model) = 0.16765221537637887\n",
      "loss_all(train_data, model) = 0.1667752004209269\n",
      "loss_all(train_data, model) = 0.1660627180844037\n",
      "loss_all(train_data, model) = 0.16574026616271084\n",
      "loss_all(train_data, model) = 0.16586210723703532\n",
      "loss_all(train_data, model) = 0.16584012074404866\n",
      "loss_all(train_data, model) = 0.16575460000633682\n",
      "loss_all(train_data, model) = 0.16575672991043966\n",
      "loss_all(train_data, model) = 0.16585582245357133\n",
      "loss_all(train_data, model) = 0.16610553641635817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.16669684143396793\n",
      "loss_all(train_data, model) = 0.1680002993846301\n",
      "loss_all(train_data, model) = 0.16946076041775865\n",
      "loss_all(train_data, model) = 0.17161127700922782\n",
      "loss_all(train_data, model) = 0.17284744676372676\n",
      "loss_all(train_data, model) = 0.17371028133131963\n",
      "loss_all(train_data, model) = 0.17431908020800174\n",
      "loss_all(train_data, model) = 0.17215522211927503\n",
      "loss_all(train_data, model) = 0.16978369885089278\n",
      "loss_all(train_data, model) = 0.1688471238614316\n",
      "loss_all(train_data, model) = 0.16745082253832325\n",
      "loss_all(train_data, model) = 0.16732623916418152\n",
      "loss_all(train_data, model) = 0.1679228546535255\n",
      "loss_all(train_data, model) = 0.16912316129029592\n",
      "loss_all(train_data, model) = 0.17091332403961482\n",
      "loss_all(train_data, model) = 0.17157686802720754\n",
      "loss_all(train_data, model) = 0.17255668361061755\n",
      "loss_all(train_data, model) = 0.17274671341841313\n",
      "loss_all(train_data, model) = 0.17384109404408088\n",
      "loss_all(train_data, model) = 0.17478280442921534\n",
      "loss_all(train_data, model) = 0.17400550506541027\n",
      "loss_all(train_data, model) = 0.17398643733186556\n",
      "loss_all(train_data, model) = 0.17377867123896285\n",
      "loss_all(train_data, model) = 0.17368176008580988\n",
      "loss_all(train_data, model) = 0.17307513002488123\n",
      "loss_all(train_data, model) = 0.1728668918604806\n",
      "loss_all(train_data, model) = 0.1727544290901763\n",
      "loss_all(train_data, model) = 0.17227449991366797\n",
      "loss_all(train_data, model) = 0.17146213917794\n",
      "loss_all(train_data, model) = 0.17022290000292123\n",
      "loss_all(train_data, model) = 0.16997294294671805\n",
      "loss_all(train_data, model) = 0.1700953628876891\n",
      "loss_all(train_data, model) = 0.16991006707709488\n",
      "loss_all(train_data, model) = 0.16976502012239075\n",
      "loss_all(train_data, model) = 0.16929586150915524\n",
      "loss_all(train_data, model) = 0.16875143369537127\n",
      "loss_all(train_data, model) = 0.16845000408338223\n",
      "loss_all(train_data, model) = 0.16859891084858353\n",
      "loss_all(train_data, model) = 0.1696347600180516\n",
      "loss_all(train_data, model) = 0.17162453971519456\n",
      "loss_all(train_data, model) = 0.17277120870079396\n",
      "loss_all(train_data, model) = 0.173313692644487\n",
      "loss_all(train_data, model) = 0.17322821007317732\n",
      "loss_all(train_data, model) = 0.1731009327891946\n",
      "loss_all(train_data, model) = 0.171057041775986\n",
      "loss_all(train_data, model) = 0.16876521106581258\n",
      "loss_all(train_data, model) = 0.16714249898109182\n",
      "loss_all(train_data, model) = 0.1667505876493591\n",
      "loss_all(train_data, model) = 0.16634888718237284\n",
      "loss_all(train_data, model) = 0.16598436352126053\n",
      "loss_all(train_data, model) = 0.16592139579753631\n",
      "loss_all(train_data, model) = 0.1658548386588997\n",
      "loss_all(train_data, model) = 0.16641200191096764\n",
      "loss_all(train_data, model) = 0.16685322534171043\n",
      "loss_all(train_data, model) = 0.16853371487909763\n",
      "loss_all(train_data, model) = 0.1698606391060709\n",
      "loss_all(train_data, model) = 0.1700749340097291\n",
      "loss_all(train_data, model) = 0.17083648359374845\n",
      "loss_all(train_data, model) = 0.1724924306305297\n",
      "loss_all(train_data, model) = 0.17503812485046735\n",
      "loss_all(train_data, model) = 0.17968799051463435\n",
      "loss_all(train_data, model) = 0.18520044584292633\n",
      "loss_all(train_data, model) = 0.18610787237236215\n",
      "loss_all(train_data, model) = 0.18310142125290443\n",
      "loss_all(train_data, model) = 0.17825772963231828\n",
      "loss_all(train_data, model) = 0.17464190927038054\n",
      "loss_all(train_data, model) = 0.170055230407214\n",
      "loss_all(train_data, model) = 0.16648794175049653\n",
      "loss_all(train_data, model) = 0.16526520214541102\n",
      "loss_all(train_data, model) = 0.1644361175431183\n",
      "loss_all(train_data, model) = 0.16375198094106191\n",
      "loss_all(train_data, model) = 0.1637517744984809\n",
      "loss_all(train_data, model) = 0.16394672983781014\n",
      "loss_all(train_data, model) = 0.16472895905785964\n",
      "loss_all(train_data, model) = 0.1661411189582595\n",
      "loss_all(train_data, model) = 0.1683744217770674\n",
      "loss_all(train_data, model) = 0.17124728965774064\n",
      "loss_all(train_data, model) = 0.17410900444086413\n",
      "loss_all(train_data, model) = 0.17673574120410884\n",
      "loss_all(train_data, model) = 0.18055766462846154\n",
      "loss_all(train_data, model) = 0.17953240615061203\n",
      "loss_all(train_data, model) = 0.17667512010310213\n",
      "loss_all(train_data, model) = 0.1724181014319071\n",
      "loss_all(train_data, model) = 0.16835203317096778\n",
      "loss_all(train_data, model) = 0.16560053184001114\n",
      "loss_all(train_data, model) = 0.16388257487664964\n",
      "loss_all(train_data, model) = 0.16487680224307688\n",
      "loss_all(train_data, model) = 0.16612792174707272\n",
      "loss_all(train_data, model) = 0.1672961908948495\n",
      "loss_all(train_data, model) = 0.16768992672296987\n",
      "loss_all(train_data, model) = 0.16685513597800872\n",
      "loss_all(train_data, model) = 0.16489214049052295\n",
      "loss_all(train_data, model) = 0.16415363966864888\n",
      "loss_all(train_data, model) = 0.16382571232793064\n",
      "loss_all(train_data, model) = 0.16374049282926934\n",
      "loss_all(train_data, model) = 0.16409867885614254\n",
      "loss_all(train_data, model) = 0.16463924278270986\n",
      "loss_all(train_data, model) = 0.16496544792876924\n",
      "loss_all(train_data, model) = 0.16515391562616785\n",
      "loss_all(train_data, model) = 0.1651523479666681\n",
      "loss_all(train_data, model) = 0.16534835095360317\n",
      "loss_all(train_data, model) = 0.16576036950018272\n",
      "loss_all(train_data, model) = 0.16619852530101478\n",
      "loss_all(train_data, model) = 0.1657784742472446\n",
      "loss_all(train_data, model) = 0.1654386246376211\n",
      "loss_all(train_data, model) = 0.16555045550065423\n",
      "loss_all(train_data, model) = 0.1657072030268677\n",
      "loss_all(train_data, model) = 0.1657093607645408\n",
      "loss_all(train_data, model) = 0.16544093851315272\n",
      "loss_all(train_data, model) = 0.16558041331754456\n",
      "loss_all(train_data, model) = 0.16585143726994322\n",
      "loss_all(train_data, model) = 0.1659023643173577\n",
      "loss_all(train_data, model) = 0.16583764669933498\n",
      "loss_all(train_data, model) = 0.1659854417466496\n",
      "loss_all(train_data, model) = 0.16572811002385068\n",
      "loss_all(train_data, model) = 0.165662248271267\n",
      "loss_all(train_data, model) = 0.16575881654617625\n",
      "loss_all(train_data, model) = 0.16585437861178504\n",
      "loss_all(train_data, model) = 0.16586868362682272\n",
      "loss_all(train_data, model) = 0.1660966841753793\n",
      "loss_all(train_data, model) = 0.16628639161180386\n",
      "loss_all(train_data, model) = 0.16616449228083013\n",
      "loss_all(train_data, model) = 0.16598429777856655\n",
      "loss_all(train_data, model) = 0.1660373214489495\n",
      "loss_all(train_data, model) = 0.1659974736902774\n",
      "loss_all(train_data, model) = 0.1660614743722875\n",
      "loss_all(train_data, model) = 0.16618363973113634\n",
      "loss_all(train_data, model) = 0.1662545550787704\n",
      "loss_all(train_data, model) = 0.16628857291319787\n",
      "loss_all(train_data, model) = 0.16646650127892257\n",
      "loss_all(train_data, model) = 0.1669676166602533\n",
      "loss_all(train_data, model) = 0.16719613233350036\n",
      "loss_all(train_data, model) = 0.16725930166920958\n",
      "loss_all(train_data, model) = 0.16765604851266513\n",
      "loss_all(train_data, model) = 0.1681460359395065\n",
      "loss_all(train_data, model) = 0.16921067757504543\n",
      "loss_all(train_data, model) = 0.1699080587404277\n",
      "loss_all(train_data, model) = 0.17117727790752726\n",
      "loss_all(train_data, model) = 0.16986011838774276\n",
      "loss_all(train_data, model) = 0.16828097767173902\n",
      "loss_all(train_data, model) = 0.16872284560661813\n",
      "loss_all(train_data, model) = 0.16795091297461193\n",
      "loss_all(train_data, model) = 0.1665417513461798\n",
      "loss_all(train_data, model) = 0.165375164756635\n",
      "loss_all(train_data, model) = 0.16517287364583638\n",
      "loss_all(train_data, model) = 0.16550768715873504\n",
      "loss_all(train_data, model) = 0.16710313587309414\n",
      "loss_all(train_data, model) = 0.16912504597008293\n",
      "loss_all(train_data, model) = 0.17020049626799624\n",
      "loss_all(train_data, model) = 0.17082873913322383\n",
      "loss_all(train_data, model) = 0.17133268703705462\n",
      "loss_all(train_data, model) = 0.17098720652212554\n",
      "loss_all(train_data, model) = 0.17082147965950029\n",
      "loss_all(train_data, model) = 0.1705629514671206\n",
      "loss_all(train_data, model) = 0.17119630981873205\n",
      "loss_all(train_data, model) = 0.17111172176630238\n",
      "loss_all(train_data, model) = 0.17065076320069048\n",
      "loss_all(train_data, model) = 0.16910933771715955\n",
      "loss_all(train_data, model) = 0.16792230959053805\n",
      "loss_all(train_data, model) = 0.16707288732466474\n",
      "loss_all(train_data, model) = 0.16690208177979704\n",
      "loss_all(train_data, model) = 0.16732299639261955\n",
      "loss_all(train_data, model) = 0.16752077718580938\n",
      "loss_all(train_data, model) = 0.16729303512109014\n",
      "loss_all(train_data, model) = 0.16673220884468395\n",
      "loss_all(train_data, model) = 0.16625884842299055\n",
      "loss_all(train_data, model) = 0.16623406837050073\n",
      "loss_all(train_data, model) = 0.16623405599951233\n",
      "loss_all(train_data, model) = 0.1662387959999635\n",
      "loss_all(train_data, model) = 0.16631504554985554\n",
      "loss_all(train_data, model) = 0.16637241365330743\n",
      "loss_all(train_data, model) = 0.16648904379657706\n",
      "loss_all(train_data, model) = 0.16655024112056574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.1666402520647609\n",
      "loss_all(train_data, model) = 0.16676462715178592\n",
      "loss_all(train_data, model) = 0.16691676587050278\n",
      "loss_all(train_data, model) = 0.16702801371250578\n",
      "loss_all(train_data, model) = 0.16709029213547838\n",
      "loss_all(train_data, model) = 0.16708529282664722\n",
      "loss_all(train_data, model) = 0.1670715092653505\n",
      "loss_all(train_data, model) = 0.16736182430286706\n",
      "loss_all(train_data, model) = 0.16764611392166975\n",
      "loss_all(train_data, model) = 0.16743723429311452\n",
      "loss_all(train_data, model) = 0.16713517468880848\n",
      "loss_all(train_data, model) = 0.1670464874965684\n",
      "loss_all(train_data, model) = 0.16713931023418624\n",
      "loss_all(train_data, model) = 0.16706700626915053\n",
      "loss_all(train_data, model) = 0.16693656810761387\n",
      "loss_all(train_data, model) = 0.16677535189977508\n",
      "loss_all(train_data, model) = 0.16662539318766634\n",
      "loss_all(train_data, model) = 0.16654606126242036\n",
      "loss_all(train_data, model) = 0.16650290276070817\n",
      "loss_all(train_data, model) = 0.16685877920259254\n",
      "loss_all(train_data, model) = 0.16721199734900832\n",
      "loss_all(train_data, model) = 0.16753410446193992\n",
      "loss_all(train_data, model) = 0.16926397269680388\n",
      "loss_all(train_data, model) = 0.16982589553498492\n",
      "loss_all(train_data, model) = 0.17039938326079523\n",
      "loss_all(train_data, model) = 0.17088986957768362\n",
      "loss_all(train_data, model) = 0.17243698571618754\n",
      "loss_all(train_data, model) = 0.17272997569581824\n",
      "loss_all(train_data, model) = 0.17210987501577282\n",
      "loss_all(train_data, model) = 0.17140526564859615\n",
      "loss_all(train_data, model) = 0.1703591573469449\n",
      "loss_all(train_data, model) = 0.16964237055093337\n",
      "loss_all(train_data, model) = 0.16677839069115818\n",
      "loss_all(train_data, model) = 0.1644947189755076\n",
      "loss_all(train_data, model) = 0.16570251938692962\n",
      "loss_all(train_data, model) = 0.17030132541364204\n",
      "loss_all(train_data, model) = 0.17789886402121066\n",
      "loss_all(train_data, model) = 0.185591715493311\n",
      "loss_all(train_data, model) = 0.19260717165234223\n",
      "loss_all(train_data, model) = 0.1994551578097105\n",
      "loss_all(train_data, model) = 0.19945695509114264\n",
      "loss_all(train_data, model) = 0.19553079770679266\n",
      "loss_all(train_data, model) = 0.19322528895288027\n",
      "loss_all(train_data, model) = 0.18913371557200623\n",
      "loss_all(train_data, model) = 0.18298219671642693\n",
      "loss_all(train_data, model) = 0.1757197023355549\n",
      "loss_all(train_data, model) = 0.16940163756742221\n",
      "loss_all(train_data, model) = 0.16534394310990536\n",
      "loss_all(train_data, model) = 0.1636752321359615\n",
      "loss_all(train_data, model) = 0.16406668014298312\n",
      "loss_all(train_data, model) = 0.1661110035204443\n",
      "loss_all(train_data, model) = 0.17072936503927866\n",
      "loss_all(train_data, model) = 0.17599505872365215\n",
      "loss_all(train_data, model) = 0.18255883677774432\n",
      "loss_all(train_data, model) = 0.18753997397450195\n",
      "loss_all(train_data, model) = 0.18775314545361535\n",
      "loss_all(train_data, model) = 0.18600608529432794\n",
      "loss_all(train_data, model) = 0.18350716822198238\n",
      "loss_all(train_data, model) = 0.1785957353009504\n",
      "loss_all(train_data, model) = 0.17304942178072977\n",
      "loss_all(train_data, model) = 0.16898322218785042\n",
      "loss_all(train_data, model) = 0.16619699202246604\n",
      "loss_all(train_data, model) = 0.16565740210645252\n",
      "loss_all(train_data, model) = 0.16481604705913538\n",
      "loss_all(train_data, model) = 0.16403679938968246\n",
      "loss_all(train_data, model) = 0.16407327107607475\n",
      "loss_all(train_data, model) = 0.16414943292402145\n",
      "loss_all(train_data, model) = 0.16384890369511748\n",
      "loss_all(train_data, model) = 0.16390491619924252\n",
      "loss_all(train_data, model) = 0.16437492420474278\n",
      "loss_all(train_data, model) = 0.16526781735276933\n",
      "loss_all(train_data, model) = 0.16551055377837767\n",
      "loss_all(train_data, model) = 0.16562063371467065\n",
      "loss_all(train_data, model) = 0.16564259510349402\n",
      "loss_all(train_data, model) = 0.16605424922350526\n",
      "loss_all(train_data, model) = 0.16642715364005195\n",
      "loss_all(train_data, model) = 0.1668916809872332\n",
      "loss_all(train_data, model) = 0.1678892382508439\n",
      "loss_all(train_data, model) = 0.16801957482840332\n",
      "loss_all(train_data, model) = 0.1675781619699658\n",
      "loss_all(train_data, model) = 0.16662397852669938\n",
      "loss_all(train_data, model) = 0.1651700311779049\n",
      "loss_all(train_data, model) = 0.16476209129538585\n",
      "loss_all(train_data, model) = 0.16502110260932557\n",
      "loss_all(train_data, model) = 0.16581811365410587\n",
      "loss_all(train_data, model) = 0.16650597010543566\n",
      "loss_all(train_data, model) = 0.16762671748191652\n",
      "loss_all(train_data, model) = 0.16940203324254974\n",
      "loss_all(train_data, model) = 0.17141061306065428\n",
      "loss_all(train_data, model) = 0.17322505846233877\n",
      "loss_all(train_data, model) = 0.17442506705438177\n",
      "loss_all(train_data, model) = 0.17504003086655326\n",
      "loss_all(train_data, model) = 0.17704497955621576\n",
      "loss_all(train_data, model) = 0.17491146032062874\n",
      "loss_all(train_data, model) = 0.17208244233039974\n",
      "loss_all(train_data, model) = 0.1703978943515288\n",
      "loss_all(train_data, model) = 0.1701050251328931\n",
      "loss_all(train_data, model) = 0.17010082456420753\n",
      "loss_all(train_data, model) = 0.16968628946504247\n",
      "loss_all(train_data, model) = 0.16820307989291516\n",
      "loss_all(train_data, model) = 0.1660606259941375\n",
      "loss_all(train_data, model) = 0.16470162066763455\n",
      "loss_all(train_data, model) = 0.16431963591994408\n",
      "loss_all(train_data, model) = 0.16533782257205284\n",
      "loss_all(train_data, model) = 0.16715985890727103\n",
      "loss_all(train_data, model) = 0.16754437627137064\n",
      "loss_all(train_data, model) = 0.16760125804672185\n",
      "loss_all(train_data, model) = 0.1675042650421033\n",
      "loss_all(train_data, model) = 0.16575984710467592\n",
      "loss_all(train_data, model) = 0.16475781199168232\n",
      "loss_all(train_data, model) = 0.16421122390212642\n",
      "loss_all(train_data, model) = 0.1639008866173346\n",
      "loss_all(train_data, model) = 0.1638912703039905\n",
      "loss_all(train_data, model) = 0.1639220523622944\n",
      "loss_all(train_data, model) = 0.16402710149295957\n",
      "loss_all(train_data, model) = 0.1647333926172998\n",
      "loss_all(train_data, model) = 0.16530527137276732\n",
      "loss_all(train_data, model) = 0.16539673956761639\n",
      "loss_all(train_data, model) = 0.16532517720557013\n",
      "loss_all(train_data, model) = 0.16484472882838855\n",
      "loss_all(train_data, model) = 0.16449718800035926\n",
      "loss_all(train_data, model) = 0.1645862378192379\n",
      "loss_all(train_data, model) = 0.1651548182775509\n",
      "loss_all(train_data, model) = 0.16637485969004132\n",
      "loss_all(train_data, model) = 0.16809984377307688\n",
      "loss_all(train_data, model) = 0.17035435696149076\n",
      "loss_all(train_data, model) = 0.17086120506760097\n",
      "loss_all(train_data, model) = 0.1702981568748405\n",
      "loss_all(train_data, model) = 0.1690398342916123\n",
      "loss_all(train_data, model) = 0.16780268010811838\n",
      "loss_all(train_data, model) = 0.16626392233156875\n",
      "loss_all(train_data, model) = 0.16517629282081897\n",
      "loss_all(train_data, model) = 0.16463189050244836\n",
      "loss_all(train_data, model) = 0.16431644256040406\n",
      "loss_all(train_data, model) = 0.16431208381313797\n",
      "loss_all(train_data, model) = 0.16426147147873782\n",
      "loss_all(train_data, model) = 0.164105801237419\n",
      "loss_all(train_data, model) = 0.16412251425727514\n",
      "loss_all(train_data, model) = 0.16410074713302567\n",
      "loss_all(train_data, model) = 0.16406527098134918\n",
      "loss_all(train_data, model) = 0.16401275992289044\n",
      "loss_all(train_data, model) = 0.1640859565486893\n",
      "loss_all(train_data, model) = 0.16433798824341608\n",
      "loss_all(train_data, model) = 0.16529382057019312\n",
      "loss_all(train_data, model) = 0.16814834037717893\n",
      "loss_all(train_data, model) = 0.1718777752617849\n",
      "loss_all(train_data, model) = 0.17579284150888963\n",
      "loss_all(train_data, model) = 0.17863481195150951\n",
      "loss_all(train_data, model) = 0.18060409857724222\n",
      "loss_all(train_data, model) = 0.17831855403547298\n",
      "loss_all(train_data, model) = 0.17135261157423404\n",
      "loss_all(train_data, model) = 0.16483636348452807\n",
      "loss_all(train_data, model) = 0.1642233288693771\n",
      "loss_all(train_data, model) = 0.1705275715803882\n",
      "loss_all(train_data, model) = 0.17799623172724424\n",
      "loss_all(train_data, model) = 0.18019476681753133\n",
      "loss_all(train_data, model) = 0.17781643508450776\n",
      "loss_all(train_data, model) = 0.1737704226272969\n",
      "loss_all(train_data, model) = 0.17060453258202468\n",
      "loss_all(train_data, model) = 0.1685531347715672\n",
      "loss_all(train_data, model) = 0.16761754292048267\n",
      "loss_all(train_data, model) = 0.16811879492377418\n",
      "loss_all(train_data, model) = 0.16901370217802553\n",
      "loss_all(train_data, model) = 0.17046018191830162\n",
      "loss_all(train_data, model) = 0.17047195568865972\n",
      "loss_all(train_data, model) = 0.1699193157752331\n",
      "loss_all(train_data, model) = 0.16965718990802112\n",
      "loss_all(train_data, model) = 0.16916086632672886\n",
      "loss_all(train_data, model) = 0.16881208526639285\n",
      "loss_all(train_data, model) = 0.168613291971108\n",
      "loss_all(train_data, model) = 0.16856318457690883\n",
      "loss_all(train_data, model) = 0.16921182961675182\n",
      "loss_all(train_data, model) = 0.17021259605215938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, model) = 0.17040986474355732\n",
      "loss_all(train_data, model) = 0.17270472679845883\n",
      "loss_all(train_data, model) = 0.17383825074160267\n",
      "loss_all(train_data, model) = 0.1721313401004357\n",
      "loss_all(train_data, model) = 0.16893008441793694\n",
      "loss_all(train_data, model) = 0.16687861764347264\n",
      "loss_all(train_data, model) = 0.16592531399636629\n",
      "loss_all(train_data, model) = 0.16514335884849496\n",
      "loss_all(train_data, model) = 0.16529275333170954\n",
      "loss_all(train_data, model) = 0.16540469619115852\n",
      "loss_all(train_data, model) = 0.16723326449367407\n",
      "loss_all(train_data, model) = 0.17003332550706404\n",
      "loss_all(train_data, model) = 0.17426997128765626\n",
      "loss_all(train_data, model) = 0.1767574976783428\n",
      "loss_all(train_data, model) = 0.18059448807641235\n",
      "loss_all(train_data, model) = 0.18255151095308325\n",
      "loss_all(train_data, model) = 0.185622520884953\n",
      "loss_all(train_data, model) = 0.18839623564683802\n",
      "loss_all(train_data, model) = 0.18582617158306045\n",
      "loss_all(train_data, model) = 0.18342235315411612\n",
      "loss_all(train_data, model) = 0.1780879031419812\n",
      "loss_all(train_data, model) = 0.17481754451688974\n",
      "loss_all(train_data, model) = 0.16830123262364377\n",
      "loss_all(train_data, model) = 0.16508452439122914\n",
      "loss_all(train_data, model) = 0.1654414311282697\n",
      "loss_all(train_data, model) = 0.16696840079079756\n",
      "loss_all(train_data, model) = 0.1692233157261711\n",
      "loss_all(train_data, model) = 0.1708887923443649\n",
      "loss_all(train_data, model) = 0.17155474624159567\n",
      "loss_all(train_data, model) = 0.16850313148661822\n",
      "loss_all(train_data, model) = 0.16585440427640752\n",
      "loss_all(train_data, model) = 0.16609211052885064\n",
      "loss_all(train_data, model) = 0.16721510770532802\n",
      "loss_all(train_data, model) = 0.16896253621475876\n",
      "loss_all(train_data, model) = 0.1695126731718337\n",
      "loss_all(train_data, model) = 0.16989188475006617\n"
     ]
    }
   ],
   "source": [
    "# can call multiple times to\n",
    "n_epochs = 5\n",
    "for i = 1:n_epochs\n",
    "    train!(loss, params(model), train_data, opt, cb=evalcb)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Array{Float32,2}:\n",
       " 44.8018  92.7857  66.6154  48.4188  …  20.0109  51.4139  25.9684  69.7778"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ŷ = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Array{Float64,2}:\n",
       " -0.271517  -0.555517  -0.547793  0.130576  …  0.0959038  -0.0875074  0.20469"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = Y .- Ŷ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"400\" height=\"400\" viewBox=\"0 0 1600 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip100\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1600\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip100)\" d=\"\n",
       "M0 1600 L1600 1600 L1600 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip101\">\n",
       "    <rect x=\"320\" y=\"160\" width=\"1121\" height=\"1121\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip100)\" d=\"\n",
       "M638.615 1124.67 L1411.02 1124.67 L1411.02 188.976 L638.615 188.976  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip102\">\n",
       "    <rect x=\"638\" y=\"188\" width=\"773\" height=\"937\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  659.666,1124.67 659.666,188.976 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  821.596,1124.67 821.596,188.976 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  983.527,1124.67 983.527,188.976 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1145.46,1124.67 1145.46,188.976 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1307.39,1124.67 1307.39,188.976 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,1124.67 1411.02,1124.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  659.666,1124.67 659.666,1113.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  821.596,1124.67 821.596,1113.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  983.527,1124.67 983.527,1113.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  1145.46,1124.67 1145.46,1113.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  1307.39,1124.67 1307.39,1113.44 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip100)\" d=\"M 0 0 M659.666 1177.43 Q645.221 1177.43 637.907 1191.69 Q630.684 1205.86 630.684 1234.38 Q630.684 1262.8 637.907 1277.06 Q645.221 1291.23 659.666 1291.23 Q674.203 1291.23 681.425 1277.06 Q688.74 1262.8 688.74 1234.38 Q688.74 1205.86 681.425 1191.69 Q674.203 1177.43 659.666 1177.43 M659.666 1162.62 Q682.906 1162.62 695.128 1181.05 Q707.443 1199.38 707.443 1234.38 Q707.443 1269.29 695.128 1287.71 Q682.906 1306.04 659.666 1306.04 Q636.425 1306.04 624.11 1287.71 Q611.888 1269.29 611.888 1234.38 Q611.888 1199.38 624.11 1181.05 Q636.425 1162.62 659.666 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M692.152 1287.62 L757.43 1287.62 L757.43 1303.36 L669.653 1303.36 L669.653 1287.62 Q680.301 1276.6 698.634 1258.08 Q717.06 1239.47 721.782 1234.1 Q730.763 1224.01 734.282 1217.06 Q737.893 1210.03 737.893 1203.27 Q737.893 1192.25 730.115 1185.3 Q722.43 1178.36 710.023 1178.36 Q701.227 1178.36 691.412 1181.42 Q681.69 1184.47 670.579 1190.67 L670.579 1171.79 Q681.875 1167.25 691.69 1164.93 Q701.504 1162.62 709.652 1162.62 Q731.134 1162.62 743.911 1173.36 Q756.689 1184.1 756.689 1202.06 Q756.689 1210.58 753.448 1218.27 Q750.3 1225.86 741.874 1236.23 Q739.56 1238.92 727.152 1251.79 Q714.745 1264.56 692.152 1287.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M817.707 1177.43 Q803.263 1177.43 795.948 1191.69 Q788.726 1205.86 788.726 1234.38 Q788.726 1262.8 795.948 1277.06 Q803.263 1291.23 817.707 1291.23 Q832.244 1291.23 839.467 1277.06 Q846.781 1262.8 846.781 1234.38 Q846.781 1205.86 839.467 1191.69 Q832.244 1177.43 817.707 1177.43 M817.707 1162.62 Q840.948 1162.62 853.17 1181.05 Q865.485 1199.38 865.485 1234.38 Q865.485 1269.29 853.17 1287.71 Q840.948 1306.04 817.707 1306.04 Q794.467 1306.04 782.152 1287.71 Q769.93 1269.29 769.93 1234.38 Q769.93 1199.38 782.152 1181.05 Q794.467 1162.62 817.707 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M925.762 1177.43 Q911.318 1177.43 904.003 1191.69 Q896.781 1205.86 896.781 1234.38 Q896.781 1262.8 904.003 1277.06 Q911.318 1291.23 925.762 1291.23 Q940.299 1291.23 947.521 1277.06 Q954.836 1262.8 954.836 1234.38 Q954.836 1205.86 947.521 1191.69 Q940.299 1177.43 925.762 1177.43 M925.762 1162.62 Q949.003 1162.62 961.225 1181.05 Q973.54 1199.38 973.54 1234.38 Q973.54 1269.29 961.225 1287.71 Q949.003 1306.04 925.762 1306.04 Q902.522 1306.04 890.207 1287.71 Q877.985 1269.29 877.985 1234.38 Q877.985 1199.38 890.207 1181.05 Q902.522 1162.62 925.762 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M887.509 1181.42 L840.287 1255.21 L887.509 1255.21 L887.509 1181.42 M882.601 1165.12 L906.12 1165.12 L906.12 1255.21 L925.842 1255.21 L925.842 1270.77 L906.12 1270.77 L906.12 1303.36 L887.509 1303.36 L887.509 1270.77 L825.102 1270.77 L825.102 1252.71 L882.601 1165.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M986.119 1177.43 Q971.675 1177.43 964.36 1191.69 Q957.138 1205.86 957.138 1234.38 Q957.138 1262.8 964.36 1277.06 Q971.675 1291.23 986.119 1291.23 Q1000.66 1291.23 1007.88 1277.06 Q1015.19 1262.8 1015.19 1234.38 Q1015.19 1205.86 1007.88 1191.69 Q1000.66 1177.43 986.119 1177.43 M986.119 1162.62 Q1009.36 1162.62 1021.58 1181.05 Q1033.9 1199.38 1033.9 1234.38 Q1033.9 1269.29 1021.58 1287.71 Q1009.36 1306.04 986.119 1306.04 Q962.879 1306.04 950.564 1287.71 Q938.342 1269.29 938.342 1234.38 Q938.342 1199.38 950.564 1181.05 Q962.879 1162.62 986.119 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1094.17 1177.43 Q1079.73 1177.43 1072.42 1191.69 Q1065.19 1205.86 1065.19 1234.38 Q1065.19 1262.8 1072.42 1277.06 Q1079.73 1291.23 1094.17 1291.23 Q1108.71 1291.23 1115.93 1277.06 Q1123.25 1262.8 1123.25 1234.38 Q1123.25 1205.86 1115.93 1191.69 Q1108.71 1177.43 1094.17 1177.43 M1094.17 1162.62 Q1117.42 1162.62 1129.64 1181.05 Q1141.95 1199.38 1141.95 1234.38 Q1141.95 1269.29 1129.64 1287.71 Q1117.42 1306.04 1094.17 1306.04 Q1070.93 1306.04 1058.62 1287.71 Q1046.4 1269.29 1046.4 1234.38 Q1046.4 1199.38 1058.62 1181.05 Q1070.93 1162.62 1094.17 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1039.02 1226.79 Q1026.43 1226.79 1019.02 1235.4 Q1011.71 1244.01 1011.71 1259.01 Q1011.71 1273.92 1019.02 1282.62 Q1026.43 1291.23 1039.02 1291.23 Q1051.62 1291.23 1058.93 1282.62 Q1066.34 1273.92 1066.34 1259.01 Q1066.34 1244.01 1058.93 1235.4 Q1051.62 1226.79 1039.02 1226.79 M1076.15 1168.18 L1076.15 1185.21 Q1069.12 1181.88 1061.89 1180.12 Q1054.76 1178.36 1047.73 1178.36 Q1029.21 1178.36 1019.39 1190.86 Q1009.67 1203.36 1008.28 1228.64 Q1013.75 1220.58 1021.99 1216.32 Q1030.23 1211.97 1040.13 1211.97 Q1060.97 1211.97 1073 1224.66 Q1085.13 1237.25 1085.13 1259.01 Q1085.13 1280.3 1072.54 1293.17 Q1059.95 1306.04 1039.02 1306.04 Q1015.04 1306.04 1002.36 1287.71 Q989.671 1269.29 989.671 1234.38 Q989.671 1201.6 1005.23 1182.16 Q1020.78 1162.62 1046.99 1162.62 Q1054.02 1162.62 1061.15 1164.01 Q1068.37 1165.4 1076.15 1168.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1145.41 1177.43 Q1130.97 1177.43 1123.65 1191.69 Q1116.43 1205.86 1116.43 1234.38 Q1116.43 1262.8 1123.65 1277.06 Q1130.97 1291.23 1145.41 1291.23 Q1159.95 1291.23 1167.17 1277.06 Q1174.49 1262.8 1174.49 1234.38 Q1174.49 1205.86 1167.17 1191.69 Q1159.95 1177.43 1145.41 1177.43 M1145.41 1162.62 Q1168.65 1162.62 1180.87 1181.05 Q1193.19 1199.38 1193.19 1234.38 Q1193.19 1269.29 1180.87 1287.71 Q1168.65 1306.04 1145.41 1306.04 Q1122.17 1306.04 1109.86 1287.71 Q1097.63 1269.29 1097.63 1234.38 Q1097.63 1199.38 1109.86 1181.05 Q1122.17 1162.62 1145.41 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1253.47 1177.43 Q1239.02 1177.43 1231.71 1191.69 Q1224.48 1205.86 1224.48 1234.38 Q1224.48 1262.8 1231.71 1277.06 Q1239.02 1291.23 1253.47 1291.23 Q1268 1291.23 1275.23 1277.06 Q1282.54 1262.8 1282.54 1234.38 Q1282.54 1205.86 1275.23 1191.69 Q1268 1177.43 1253.47 1177.43 M1253.47 1162.62 Q1276.71 1162.62 1288.93 1181.05 Q1301.24 1199.38 1301.24 1234.38 Q1301.24 1269.29 1288.93 1287.71 Q1276.71 1306.04 1253.47 1306.04 Q1230.23 1306.04 1217.91 1287.71 Q1205.69 1269.29 1205.69 1234.38 Q1205.69 1199.38 1217.91 1181.05 Q1230.23 1162.62 1253.47 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1199.33 1237.71 Q1186 1237.71 1178.31 1244.84 Q1170.72 1251.97 1170.72 1264.47 Q1170.72 1276.97 1178.31 1284.1 Q1186 1291.23 1199.33 1291.23 Q1212.67 1291.23 1220.35 1284.1 Q1228.04 1276.88 1228.04 1264.47 Q1228.04 1251.97 1220.35 1244.84 Q1212.76 1237.71 1199.33 1237.71 M1180.63 1229.75 Q1168.59 1226.79 1161.83 1218.55 Q1155.17 1210.3 1155.17 1198.45 Q1155.17 1181.88 1166.93 1172.25 Q1178.78 1162.62 1199.33 1162.62 Q1219.98 1162.62 1231.74 1172.25 Q1243.5 1181.88 1243.5 1198.45 Q1243.5 1210.3 1236.74 1218.55 Q1230.07 1226.79 1218.13 1229.75 Q1231.65 1232.9 1239.15 1242.06 Q1246.74 1251.23 1246.74 1264.47 Q1246.74 1284.56 1234.43 1295.3 Q1222.2 1306.04 1199.33 1306.04 Q1176.46 1306.04 1164.15 1295.3 Q1151.93 1284.56 1151.93 1264.47 Q1151.93 1251.23 1159.52 1242.06 Q1167.11 1232.9 1180.63 1229.75 M1173.78 1200.21 Q1173.78 1210.95 1180.44 1216.97 Q1187.2 1222.99 1199.33 1222.99 Q1211.37 1222.99 1218.13 1216.97 Q1224.98 1210.95 1224.98 1200.21 Q1224.98 1189.47 1218.13 1183.45 Q1211.37 1177.43 1199.33 1177.43 Q1187.2 1177.43 1180.44 1183.45 Q1173.78 1189.47 1173.78 1200.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1307.02 1177.43 Q1292.57 1177.43 1285.26 1191.69 Q1278.04 1205.86 1278.04 1234.38 Q1278.04 1262.8 1285.26 1277.06 Q1292.57 1291.23 1307.02 1291.23 Q1321.55 1291.23 1328.78 1277.06 Q1336.09 1262.8 1336.09 1234.38 Q1336.09 1205.86 1328.78 1191.69 Q1321.55 1177.43 1307.02 1177.43 M1307.02 1162.62 Q1330.26 1162.62 1342.48 1181.05 Q1354.8 1199.38 1354.8 1234.38 Q1354.8 1269.29 1342.48 1287.71 Q1330.26 1306.04 1307.02 1306.04 Q1283.78 1306.04 1271.46 1287.71 Q1259.24 1269.29 1259.24 1234.38 Q1259.24 1199.38 1271.46 1181.05 Q1283.78 1162.62 1307.02 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1415.07 1177.43 Q1400.63 1177.43 1393.31 1191.69 Q1386.09 1205.86 1386.09 1234.38 Q1386.09 1262.8 1393.31 1277.06 Q1400.63 1291.23 1415.07 1291.23 Q1429.61 1291.23 1436.83 1277.06 Q1444.15 1262.8 1444.15 1234.38 Q1444.15 1205.86 1436.83 1191.69 Q1429.61 1177.43 1415.07 1177.43 M1415.07 1162.62 Q1438.31 1162.62 1450.54 1181.05 Q1462.85 1199.38 1462.85 1234.38 Q1462.85 1269.29 1450.54 1287.71 Q1438.31 1306.04 1415.07 1306.04 Q1391.83 1306.04 1379.52 1287.71 Q1367.3 1269.29 1367.3 1234.38 Q1367.3 1199.38 1379.52 1181.05 Q1391.83 1162.62 1415.07 1162.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  638.615,996.078 1411.02,996.078 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  638.615,843.193 1411.02,843.193 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  638.615,690.308 1411.02,690.308 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  638.615,537.423 1411.02,537.423 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  638.615,384.538 1411.02,384.538 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip102)\" style=\"stroke:#000000; stroke-width:8; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  638.615,231.653 1411.02,231.653 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,1124.67 638.615,188.976 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,996.078 647.884,996.078 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,843.193 647.884,843.193 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,690.308 647.884,690.308 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,537.423 647.884,537.423 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,384.538 647.884,384.538 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  638.615,231.653 647.884,231.653 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip100)\" d=\"M 0 0 M244.894 997.883 L363.597 997.883 L363.597 1013.62 L244.894 1013.62 L244.894 997.883 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M387.116 1049.46 L417.671 1049.46 L417.671 943.995 L384.431 950.661 L384.431 933.625 L417.486 926.958 L436.19 926.958 L436.19 1049.46 L466.745 1049.46 L466.745 1065.2 L387.116 1065.2 L387.116 1049.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M487.023 1041.68 L506.56 1041.68 L506.56 1065.2 L487.023 1065.2 L487.023 1041.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M566.837 939.273 Q552.393 939.273 545.078 953.532 Q537.856 967.698 537.856 996.217 Q537.856 1024.64 545.078 1038.9 Q552.393 1053.07 566.837 1053.07 Q581.374 1053.07 588.596 1038.9 Q595.911 1024.64 595.911 996.217 Q595.911 967.698 588.596 953.532 Q581.374 939.273 566.837 939.273 M566.837 924.458 Q590.078 924.458 602.3 942.884 Q614.615 961.217 614.615 996.217 Q614.615 1031.12 602.3 1049.55 Q590.078 1067.88 566.837 1067.88 Q543.597 1067.88 531.282 1049.55 Q519.06 1031.12 519.06 996.217 Q519.06 961.217 531.282 942.884 Q543.597 924.458 566.837 924.458 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M243.969 844.999 L362.672 844.999 L362.672 860.739 L243.969 860.739 L243.969 844.999 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M422.949 786.388 Q408.505 786.388 401.19 800.647 Q393.968 814.814 393.968 843.332 Q393.968 871.758 401.19 886.017 Q408.505 900.183 422.949 900.183 Q437.486 900.183 444.708 886.017 Q452.023 871.758 452.023 843.332 Q452.023 814.814 444.708 800.647 Q437.486 786.388 422.949 786.388 M422.949 771.573 Q446.19 771.573 458.412 789.999 Q470.727 808.332 470.727 843.332 Q470.727 878.239 458.412 896.665 Q446.19 914.998 422.949 914.998 Q399.708 914.998 387.394 896.665 Q375.172 878.239 375.172 843.332 Q375.172 808.332 387.394 789.999 Q399.708 771.573 422.949 771.573 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M491.004 888.795 L510.541 888.795 L510.541 912.313 L491.004 912.313 L491.004 888.795 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M531.004 774.073 L604.43 774.073 L604.43 789.814 L548.134 789.814 L548.134 823.702 Q552.208 822.313 556.282 821.665 Q560.356 820.925 564.43 820.925 Q587.578 820.925 601.096 833.61 Q614.615 846.295 614.615 867.961 Q614.615 890.276 600.726 902.683 Q586.837 914.998 561.559 914.998 Q552.856 914.998 543.782 913.517 Q534.8 912.035 525.171 909.072 L525.171 890.276 Q533.504 894.813 542.393 897.035 Q551.282 899.257 561.189 899.257 Q577.207 899.257 586.559 890.832 Q595.911 882.406 595.911 867.961 Q595.911 853.517 586.559 845.091 Q577.207 836.665 561.189 836.665 Q553.689 836.665 546.189 838.332 Q538.782 839.999 531.004 843.517 L531.004 774.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M418.968 633.503 Q404.523 633.503 397.208 647.762 Q389.986 661.929 389.986 690.447 Q389.986 718.873 397.208 733.132 Q404.523 747.298 418.968 747.298 Q433.505 747.298 440.727 733.132 Q448.041 718.873 448.041 690.447 Q448.041 661.929 440.727 647.762 Q433.505 633.503 418.968 633.503 M418.968 618.688 Q442.208 618.688 454.43 637.114 Q466.745 655.447 466.745 690.447 Q466.745 725.354 454.43 743.78 Q442.208 762.113 418.968 762.113 Q395.727 762.113 383.412 743.78 Q371.19 725.354 371.19 690.447 Q371.19 655.447 383.412 637.114 Q395.727 618.688 418.968 618.688 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M487.023 735.91 L506.56 735.91 L506.56 759.428 L487.023 759.428 L487.023 735.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M566.837 633.503 Q552.393 633.503 545.078 647.762 Q537.856 661.929 537.856 690.447 Q537.856 718.873 545.078 733.132 Q552.393 747.298 566.837 747.298 Q581.374 747.298 588.596 733.132 Q595.911 718.873 595.911 690.447 Q595.911 661.929 588.596 647.762 Q581.374 633.503 566.837 633.503 M566.837 618.688 Q590.078 618.688 602.3 637.114 Q614.615 655.447 614.615 690.447 Q614.615 725.354 602.3 743.78 Q590.078 762.113 566.837 762.113 Q543.597 762.113 531.282 743.78 Q519.06 725.354 519.06 690.447 Q519.06 655.447 531.282 637.114 Q543.597 618.688 566.837 618.688 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M422.949 480.618 Q408.505 480.618 401.19 494.877 Q393.968 509.044 393.968 537.562 Q393.968 565.988 401.19 580.247 Q408.505 594.414 422.949 594.414 Q437.486 594.414 444.708 580.247 Q452.023 565.988 452.023 537.562 Q452.023 509.044 444.708 494.877 Q437.486 480.618 422.949 480.618 M422.949 465.803 Q446.19 465.803 458.412 484.229 Q470.727 502.562 470.727 537.562 Q470.727 572.469 458.412 590.895 Q446.19 609.228 422.949 609.228 Q399.708 609.228 387.394 590.895 Q375.172 572.469 375.172 537.562 Q375.172 502.562 387.394 484.229 Q399.708 465.803 422.949 465.803 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M491.004 583.025 L510.541 583.025 L510.541 606.543 L491.004 606.543 L491.004 583.025 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M531.004 468.303 L604.43 468.303 L604.43 484.044 L548.134 484.044 L548.134 517.932 Q552.208 516.544 556.282 515.895 Q560.356 515.155 564.43 515.155 Q587.578 515.155 601.096 527.84 Q614.615 540.525 614.615 562.191 Q614.615 584.506 600.726 596.914 Q586.837 609.228 561.559 609.228 Q552.856 609.228 543.782 607.747 Q534.8 606.265 525.171 603.302 L525.171 584.506 Q533.504 589.043 542.393 591.265 Q551.282 593.488 561.189 593.488 Q577.207 593.488 586.559 585.062 Q595.911 576.636 595.911 562.191 Q595.911 547.747 586.559 539.321 Q577.207 530.895 561.189 530.895 Q553.689 530.895 546.189 532.562 Q538.782 534.229 531.004 537.747 L531.004 468.303 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M387.116 437.918 L417.671 437.918 L417.671 332.455 L384.431 339.122 L384.431 322.085 L417.486 315.418 L436.19 315.418 L436.19 437.918 L466.745 437.918 L466.745 453.658 L387.116 453.658 L387.116 437.918 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M487.023 430.14 L506.56 430.14 L506.56 453.658 L487.023 453.658 L487.023 430.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M566.837 327.733 Q552.393 327.733 545.078 341.992 Q537.856 356.159 537.856 384.677 Q537.856 413.103 545.078 427.362 Q552.393 441.529 566.837 441.529 Q581.374 441.529 588.596 427.362 Q595.911 413.103 595.911 384.677 Q595.911 356.159 588.596 341.992 Q581.374 327.733 566.837 327.733 M566.837 312.918 Q590.078 312.918 602.3 331.344 Q614.615 349.677 614.615 384.677 Q614.615 419.584 602.3 438.01 Q590.078 456.343 566.837 456.343 Q543.597 456.343 531.282 438.01 Q519.06 419.584 519.06 384.677 Q519.06 349.677 531.282 331.344 Q543.597 312.918 566.837 312.918 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M391.097 285.033 L421.653 285.033 L421.653 179.57 L388.412 186.237 L388.412 169.2 L421.468 162.533 L440.171 162.533 L440.171 285.033 L470.727 285.033 L470.727 300.773 L391.097 300.773 L391.097 285.033 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M491.004 277.255 L510.541 277.255 L510.541 300.773 L491.004 300.773 L491.004 277.255 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M531.004 162.533 L604.43 162.533 L604.43 178.274 L548.134 178.274 L548.134 212.163 Q552.208 210.774 556.282 210.126 Q560.356 209.385 564.43 209.385 Q587.578 209.385 601.096 222.07 Q614.615 234.755 614.615 256.422 Q614.615 278.736 600.726 291.144 Q586.837 303.458 561.559 303.458 Q552.856 303.458 543.782 301.977 Q534.8 300.495 525.171 297.533 L525.171 278.736 Q533.504 283.273 542.393 285.496 Q551.282 287.718 561.189 287.718 Q577.207 287.718 586.559 279.292 Q595.911 270.866 595.911 256.422 Q595.911 241.977 586.559 233.551 Q577.207 225.125 561.189 225.125 Q553.689 225.125 546.189 226.792 Q538.782 228.459 531.004 231.977 L531.004 162.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip102)\" style=\"stroke:#009af9; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  660.475,664.462 661.285,619.1 662.095,885.811 662.904,377.852 663.714,712.836 664.524,695.955 665.333,908.201 666.143,639.907 666.953,714.761 667.762,646.993 \n",
       "  668.572,758.186 669.381,705.576 670.191,670.796 671.001,545.635 671.81,946.55 672.62,876.979 673.43,614.613 674.239,650.386 675.049,759.789 675.859,496.616 \n",
       "  676.668,633.243 677.478,669.929 678.288,508.203 679.097,802.819 679.907,520.065 680.717,714.538 681.526,610.839 682.336,437.33 683.146,667.076 683.955,539.915 \n",
       "  684.765,455.706 685.575,517.426 686.384,718.996 687.194,795.194 688.003,716.583 688.813,470.813 689.623,557.803 690.432,624.925 691.242,541.475 692.052,491.644 \n",
       "  692.861,891.647 693.671,701.137 694.481,813.267 695.29,591.551 696.1,586.628 696.91,754.145 697.719,750.117 698.529,575.586 699.339,546.288 700.148,688.06 \n",
       "  700.958,561.844 701.768,920.292 702.577,766.275 703.387,591.87 704.197,668.854 705.006,672.27 705.816,602.071 706.626,863.257 707.435,919.953 708.245,515.27 \n",
       "  709.054,705.401 709.864,630.731 710.674,648.533 711.483,656.385 712.293,684.404 713.103,739.959 713.912,858.515 714.722,589.656 715.532,613.603 716.341,756.025 \n",
       "  717.151,832.968 717.961,604.15 718.77,588.85 719.58,826.936 720.39,641.878 721.199,735.72 722.009,550.652 722.819,643.765 723.628,802.744 724.438,944.577 \n",
       "  725.248,659.235 726.057,686.6 726.867,612.246 727.676,655.983 728.486,875.337 729.296,565.833 730.105,510.989 730.915,745.955 731.725,422.255 732.534,526.706 \n",
       "  733.344,630.16 734.154,485.882 734.963,637.289 735.773,715.877 736.583,767.718 737.392,523.058 738.202,552.459 739.012,614.277 739.821,471.927 740.631,790.857 \n",
       "  741.441,680.585 742.25,845.51 743.06,709.593 743.87,1006.41 744.679,554.917 745.489,608.88 746.299,743.251 747.108,601.66 747.918,791.231 748.727,772.791 \n",
       "  749.537,535.332 750.347,687.329 751.156,746.733 751.966,660.183 752.776,697.062 753.585,763.34 754.395,601.431 755.205,831.807 756.014,562.448 756.824,552.604 \n",
       "  757.634,704.379 758.443,679.058 759.253,709.792 760.063,496.522 760.872,752.953 761.682,987.73 762.492,682.99 763.301,700.201 764.111,684.215 764.921,693.647 \n",
       "  765.73,759.134 766.54,543.328 767.349,611.01 768.159,606.975 768.969,899.257 769.778,732.88 770.588,276.941 771.398,772.644 772.207,680.724 773.017,748.522 \n",
       "  773.827,851.287 774.636,553.054 775.446,490.965 776.256,644.353 777.065,630.07 777.875,587.478 778.685,541.553 779.494,905.882 780.304,865.905 781.114,745.73 \n",
       "  781.923,646.108 782.733,549.046 783.543,919.715 784.352,771.229 785.162,847.178 785.972,758.416 786.781,820.639 787.591,408.608 788.4,839.364 789.21,651.181 \n",
       "  790.02,529.663 790.829,532.847 791.639,778.072 792.449,489.562 793.258,719.706 794.068,731.418 794.878,386.974 795.687,646.647 796.497,768.668 797.307,773.282 \n",
       "  798.116,390.078 798.926,697.272 799.736,634.075 800.545,609.16 801.355,899.025 802.165,875.277 802.974,581.699 803.784,705.504 804.594,511.916 805.403,765.866 \n",
       "  806.213,635.087 807.022,606.468 807.832,641.244 808.642,617.897 809.451,618.539 810.261,805.464 811.071,519.768 811.88,724.165 812.69,789.631 813.5,484.452 \n",
       "  814.309,777.474 815.119,412.533 815.929,755.235 816.738,436.945 817.548,483.876 818.358,925.654 819.167,695.993 819.977,916.254 820.787,826.136 821.596,613.555 \n",
       "  822.406,750.054 823.216,523.442 824.025,506.217 824.835,601.83 825.645,854.613 826.454,848.118 827.264,921.282 828.073,879.302 828.883,472.02 829.693,599.888 \n",
       "  830.502,742.188 831.312,748.913 832.122,871.513 832.931,837.479 833.741,690.473 834.551,721.091 835.36,650.582 836.17,904.523 836.98,726.692 837.789,598.506 \n",
       "  838.599,855.993 839.409,765.193 840.218,502.159 841.028,921.717 841.838,647.726 842.647,729.923 843.457,559.758 844.267,758.693 845.076,708.188 845.886,611.756 \n",
       "  846.695,776.773 847.505,825.676 848.315,921.286 849.124,699.555 849.934,665.741 850.744,793.384 851.553,614.11 852.363,630.767 853.173,707.982 853.982,649.45 \n",
       "  854.792,410.894 855.602,445.884 856.411,958.97 857.221,489.929 858.031,752.207 858.84,504.358 859.65,779.954 860.46,835.832 861.269,497.727 862.079,602.665 \n",
       "  862.889,782.62 863.698,545.929 864.508,785.032 865.318,669.779 866.127,677.843 866.937,551.717 867.746,720.994 868.556,632.127 869.366,599.513 870.175,726.8 \n",
       "  870.985,451.808 871.795,575.353 872.604,893.067 873.414,708.658 874.224,530.219 875.033,578.55 875.843,511.049 876.653,668.902 877.462,767.782 878.272,670.122 \n",
       "  879.082,698.622 879.891,541.676 880.701,765.038 881.511,739.273 882.32,740.088 883.13,829.266 883.94,459.688 884.749,596.462 885.559,637.635 886.368,427.738 \n",
       "  887.178,215.458 887.988,695.588 888.797,678.389 889.607,714.091 890.417,799.154 891.226,574.147 892.036,847.885 892.846,552.408 893.655,614.636 894.465,669.649 \n",
       "  895.275,765.573 896.084,674.569 896.894,406.226 897.704,775.922 898.513,613.567 899.323,740.802 900.133,649.006 900.942,697.76 901.752,552.321 902.562,783.154 \n",
       "  903.371,750.057 904.181,1008.33 904.991,851.053 905.8,730.276 906.61,473.608 907.419,966.875 908.229,728.918 909.039,825.103 909.848,600.992 910.658,599.579 \n",
       "  911.468,554.626 912.277,545.25 913.087,490.692 913.897,536.635 914.706,730.46 915.516,756.519 916.326,680.965 917.135,454.19 917.945,848.047 918.755,786.271 \n",
       "  919.564,693.156 920.374,535.304 921.184,589.829 921.993,636.791 922.803,625.035 923.613,617.554 924.422,511.354 925.232,718.634 926.041,582.313 926.851,548.881 \n",
       "  927.661,725.657 928.47,956.972 929.28,703.288 930.09,711.985 930.899,676.477 931.709,797.482 932.519,601.127 933.328,730.09 934.138,781.319 934.948,465.177 \n",
       "  935.757,730.491 936.567,627.178 937.377,692.307 938.186,471.173 938.996,578.971 939.806,828.063 940.615,533.743 941.425,879.096 942.235,624.721 943.044,558.737 \n",
       "  943.854,714.064 944.663,791.924 945.473,543.125 946.283,608.445 947.092,533.935 947.902,720.06 948.712,619.211 949.521,710.453 950.331,828.073 951.141,651.411 \n",
       "  951.95,578.227 952.76,752.97 953.57,804.085 954.379,731.722 955.189,726.855 955.999,900.2 956.808,493.564 957.618,572.122 958.428,671.219 959.237,746.965 \n",
       "  960.047,898.356 960.857,818.716 961.666,661.894 962.476,620.685 963.286,594.622 964.095,695.783 964.905,784.476 965.714,712.338 966.524,632.166 967.334,637.12 \n",
       "  968.143,622.742 968.953,634.754 969.763,754.509 970.572,521.165 971.382,626.681 972.192,862.699 973.001,838.847 973.811,715.935 974.621,788.057 975.43,758.112 \n",
       "  976.24,533.131 977.05,603.707 977.859,833.273 978.669,697.273 979.479,573.884 980.288,610.048 981.098,697.532 981.908,641.126 982.717,886.846 983.527,683.365 \n",
       "  984.336,596.758 985.146,697.331 985.956,763.619 986.765,1098.19 987.575,702.569 988.385,771.504 989.194,745.661 990.004,766.94 990.814,666.736 991.623,398.173 \n",
       "  992.433,930.755 993.243,531.972 994.052,760.13 994.862,436.52 995.672,761.095 996.481,608.508 997.291,663.224 998.101,1038.61 998.91,576.927 999.72,719.504 \n",
       "  1000.53,648.112 1001.34,783.355 1002.15,761.63 1002.96,569.46 1003.77,979.465 1004.58,855.725 1005.39,632.644 1006.2,641.811 1007.01,512.289 1007.82,536.851 \n",
       "  1008.63,677.234 1009.44,706.558 1010.25,626.693 1011.06,819.376 1011.86,758.858 1012.67,694.852 1013.48,663.425 1014.29,842.602 1015.1,618.922 1015.91,530.415 \n",
       "  1016.72,674.272 1017.53,658.238 1018.34,749.814 1019.15,789.71 1019.96,517.977 1020.77,665.721 1021.58,468.802 1022.39,297.789 1023.2,685.351 1024.01,804.211 \n",
       "  1024.82,721.546 1025.63,584.396 1026.44,779.434 1027.25,483.858 1028.06,861.603 1028.87,643.14 1029.68,697.639 1030.49,521.324 1031.3,524.53 1032.11,571.307 \n",
       "  1032.92,555.635 1033.73,717.561 1034.53,721.135 1035.34,610.206 1036.15,704.375 1036.96,691.282 1037.77,817.437 1038.58,844.532 1039.39,686.41 1040.2,702.917 \n",
       "  1041.01,699.823 1041.82,682.665 1042.63,606.878 1043.44,652.059 1044.25,639.379 1045.06,670.055 1045.87,676.288 1046.68,541.415 1047.49,835.416 1048.3,612.649 \n",
       "  1049.11,711.721 1049.92,324.428 1050.73,510.459 1051.54,645.159 1052.35,566.029 1053.16,727.561 1053.97,769.751 1054.78,658.714 1055.59,566.915 1056.4,639.808 \n",
       "  1057.21,536.695 1058.01,650.743 1058.82,621.084 1059.63,624.448 1060.44,664.458 1061.25,832.335 1062.06,814.386 1062.87,824.023 1063.68,579.951 1064.49,820.371 \n",
       "  1065.3,877.671 1066.11,765.435 1066.92,661.602 1067.73,620.047 1068.54,820.974 1069.35,837.439 1070.16,448.056 1070.97,534.34 1071.78,669.302 1072.59,829.441 \n",
       "  1073.4,602.278 1074.21,757.284 1075.02,579.296 1075.83,606.379 1076.64,791.235 1077.45,971.464 1078.26,838.324 1079.07,728.433 1079.88,608.41 1080.69,788.751 \n",
       "  1081.49,691.851 1082.3,568.84 1083.11,665.42 1083.92,455.211 1084.73,742.852 1085.54,606.958 1086.35,732.959 1087.16,542.94 1087.97,730.35 1088.78,729.677 \n",
       "  1089.59,698.452 1090.4,685.968 1091.21,645.322 1092.02,847.406 1092.83,557.902 1093.64,508.183 1094.45,642.978 1095.26,676.659 1096.07,784.788 1096.88,684.043 \n",
       "  1097.69,618.97 1098.5,663.6 1099.31,819.997 1100.12,667.641 1100.93,637.456 1101.74,367.619 1102.55,777.687 1103.36,575.177 1104.17,512.82 1104.97,596.893 \n",
       "  1105.78,616.182 1106.59,889.021 1107.4,831.341 1108.21,657.114 1109.02,809.97 1109.83,571.961 1110.64,801.239 1111.45,612.28 1112.26,587.47 1113.07,792.862 \n",
       "  1113.88,581.282 1114.69,666.042 1115.5,786.14 1116.31,602.842 1117.12,520.567 1117.93,540.037 1118.74,573.278 1119.55,702.555 1120.36,432.057 1121.17,645.439 \n",
       "  1121.98,529.506 1122.79,445.292 1123.6,540.922 1124.41,585.55 1125.22,795.395 1126.03,608.11 1126.84,678.831 1127.65,866.526 1128.45,663.903 1129.26,852.939 \n",
       "  1130.07,531.84 1130.88,462.335 1131.69,808.509 1132.5,851.334 1133.31,702.814 1134.12,488.963 1134.93,781.511 1135.74,731.259 1136.55,732.938 1137.36,705.469 \n",
       "  1138.17,641.314 1138.98,492.339 1139.79,658.166 1140.6,582.519 1141.41,588.694 1142.22,487.921 1143.03,771.318 1143.84,829.842 1144.65,743.151 1145.46,483.718 \n",
       "  1146.27,910.477 1147.08,708.746 1147.89,743.683 1148.7,728.108 1149.51,446.209 1150.32,888.804 1151.13,601.9 1151.93,329.677 1152.74,702.466 1153.55,666.733 \n",
       "  1154.36,586.523 1155.17,643.66 1155.98,725.375 1156.79,767.447 1157.6,702.45 1158.41,496.774 1159.22,607.028 1160.03,608.203 1160.84,922.944 1161.65,951.346 \n",
       "  1162.46,339.808 1163.27,855.07 1164.08,584.002 1164.89,798.7 1165.7,678.839 1166.51,832.029 1167.32,666.636 1168.13,731.738 1168.94,590.923 1169.75,823.645 \n",
       "  1170.56,726.073 1171.37,626.067 1172.18,610.247 1172.99,704.661 1173.8,636.788 1174.6,702.594 1175.41,972.47 1176.22,751.545 1177.03,641.712 1177.84,403.636 \n",
       "  1178.65,411.093 1179.46,493.839 1180.27,834.838 1181.08,790.272 1181.89,849.746 1182.7,769.228 1183.51,409.012 1184.32,749.654 1185.13,777.443 1185.94,682.159 \n",
       "  1186.75,591.502 1187.56,707.618 1188.37,637.176 1189.18,555.358 1189.99,592.222 1190.8,517.473 1191.61,473.53 1192.42,746.211 1193.23,690.93 1194.04,632.358 \n",
       "  1194.85,876.127 1195.66,716.654 1196.47,644.076 1197.28,729.71 1198.08,530.092 1198.89,642.125 1199.7,624.363 1200.51,820.645 1201.32,682.895 1202.13,812.726 \n",
       "  1202.94,506.983 1203.75,605.189 1204.56,650.446 1205.37,572.943 1206.18,767.724 1206.99,655.376 1207.8,563.803 1208.61,739.885 1209.42,718.772 1210.23,564.798 \n",
       "  1211.04,659.72 1211.85,623.294 1212.66,758.575 1213.47,779.498 1214.28,673.135 1215.09,584.345 1215.9,682.371 1216.71,1021.36 1217.52,571.471 1218.33,1002.9 \n",
       "  1219.14,662.961 1219.95,738.852 1220.76,638.09 1221.56,719.031 1222.37,606.451 1223.18,834.866 1223.99,685.709 1224.8,738.489 1225.61,628.801 1226.42,799.714 \n",
       "  1227.23,398.075 1228.04,652.86 1228.85,959.686 1229.66,557.157 1230.47,640.124 1231.28,573.194 1232.09,639.727 1232.9,521.669 1233.71,1052.81 1234.52,716.166 \n",
       "  1235.33,765.894 1236.14,891.454 1236.95,589.636 1237.76,919.532 1238.57,704.205 1239.38,711.176 1240.19,603.791 1241,947.328 1241.81,672.677 1242.62,629.814 \n",
       "  1243.43,728.456 1244.24,522.833 1245.04,800.788 1245.85,712.927 1246.66,682.752 1247.47,694.62 1248.28,758.465 1249.09,672.87 1249.9,641.619 1250.71,623.711 \n",
       "  1251.52,479.017 1252.33,880.112 1253.14,668.132 1253.95,565.485 1254.76,520.299 1255.57,690.968 1256.38,669.783 1257.19,687.083 1258,973.52 1258.81,641.291 \n",
       "  1259.62,782.691 1260.43,713.849 1261.24,728.96 1262.05,613.831 1262.86,760.255 1263.67,650.175 1264.48,836.108 1265.29,804.951 1266.1,653.469 1266.91,708.233 \n",
       "  1267.72,542.576 1268.52,731.216 1269.33,620.667 1270.14,615.782 1270.95,655.197 1271.76,766.652 1272.57,699.74 1273.38,649.181 1274.19,784.577 1275,729.837 \n",
       "  1275.81,610.212 1276.62,743.371 1277.43,652.495 1278.24,655.185 1279.05,803.686 1279.86,811.445 1280.67,771.529 1281.48,802.883 1282.29,754.649 1283.1,846.356 \n",
       "  1283.91,561.195 1284.72,653.182 1285.53,655.656 1286.34,477.99 1287.15,780.122 1287.96,829.96 1288.77,552.54 1289.58,518.525 1290.39,876.746 1291.19,575.436 \n",
       "  1292,670.832 1292.81,520.591 1293.62,605.198 1294.43,617.944 1295.24,556.784 1296.05,779.571 1296.86,808.6 1297.67,792.898 1298.48,571.352 1299.29,527.062 \n",
       "  1300.1,806.978 1300.91,586.866 1301.72,437.109 1302.53,714.416 1303.34,719.853 1304.15,572.622 1304.96,638.26 1305.77,754.508 1306.58,696.655 1307.39,535.443 \n",
       "  1308.2,634.881 1309.01,700.958 1309.82,738.47 1310.63,806.771 1311.44,768.043 1312.25,643.235 1313.06,903.01 1313.87,940.527 1314.67,778.47 1315.48,790.049 \n",
       "  1316.29,631.348 1317.1,699.203 1317.91,586.971 1318.72,616.547 1319.53,886.389 1320.34,656.219 1321.15,671.074 1321.96,679.863 1322.77,971.581 1323.58,576.14 \n",
       "  1324.39,835.348 1325.2,718.812 1326.01,567.496 1326.82,835.049 1327.63,761.007 1328.44,617.1 1329.25,803.218 1330.06,674.794 1330.87,559.192 1331.68,814.031 \n",
       "  1332.49,556.219 1333.3,486.31 1334.11,712.749 1334.92,725.648 1335.73,696.064 1336.54,714.998 1337.35,644.929 1338.15,460.008 1338.96,662.202 1339.77,752.76 \n",
       "  1340.58,859.64 1341.39,519.103 1342.2,834.606 1343.01,537.038 1343.82,651.112 1344.63,638.129 1345.44,783.772 1346.25,950.995 1347.06,595.817 1347.87,775.781 \n",
       "  1348.68,633.934 1349.49,744.478 1350.3,586.709 1351.11,655.746 1351.92,593.688 1352.73,646.869 1353.54,934.243 1354.35,412.412 1355.16,461.666 1355.97,601.653 \n",
       "  1356.78,530.064 1357.59,833.577 1358.4,699.433 1359.21,707.792 1360.02,622.219 1360.83,379.904 1361.63,782.475 1362.44,733.953 1363.25,800.079 1364.06,556.628 \n",
       "  1364.87,726.844 1365.68,835.227 1366.49,712.967 1367.3,505.332 1368.11,921.391 1368.92,898.684 1369.73,695.464 1370.54,651.288 1371.35,437.584 1372.16,855.423 \n",
       "  1372.97,738.92 1373.78,679.937 1374.59,496.776 1375.4,770.202 1376.21,612.717 1377.02,648.616 1377.83,679.158 1378.64,600.314 1379.45,645.984 1380.26,560.478 \n",
       "  1381.07,726.574 1381.88,702.144 1382.69,643.754 1383.5,670.406 1384.31,979.962 1385.11,734.206 1385.92,503.451 1386.73,722.05 1387.54,491.234 1388.35,669.381 \n",
       "  1389.16,631.783 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip100)\" d=\"\n",
       "M1095.38 704.006 L1385.28 704.006 L1385.28 220.166 L1095.38 220.166  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#000000; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  1095.38,704.006 1385.28,704.006 1385.28,220.166 1095.38,220.166 1095.38,704.006 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip100)\" style=\"stroke:#009af9; stroke-width:16; stroke-opacity:1; fill:none\" points=\"\n",
       "  1103.96,462.086 1155.46,462.086 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip100)\" d=\"M 0 0 M1219.41 540.836 Q1212.19 559.354 1205.34 565.002 Q1198.48 570.65 1187 570.65 L1173.39 570.65 L1173.39 556.391 L1183.39 556.391 Q1190.43 556.391 1194.32 553.058 Q1198.21 549.725 1202.93 537.317 L1205.98 529.54 L1164.04 427.503 L1182.09 427.503 L1214.5 508.614 L1246.91 427.503 L1264.96 427.503 L1219.41 540.836 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip100)\" d=\"M 0 0 M1288.48 515.466 L1319.04 515.466 L1319.04 410.003 L1285.8 416.67 L1285.8 399.633 L1318.85 392.966 L1337.56 392.966 L1337.56 515.466 L1368.11 515.466 L1368.11 531.206 L1288.48 531.206 L1288.48 515.466 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plots.scalefontsizes(1)\n",
    "gr()\n",
    "Plots.plot(error[100:1000], size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7466639712164316"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.35417838700387"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3262935147358401"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs.(error))/length(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.scalefontsizes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step: save training data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling BSON [fbb218c0-5317-5bc6-957e-2ee96dd4b1f0]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    }
   ],
   "source": [
    "using BSON: @save\n",
    "@save \"lidar_model_1.bson\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save \"training_data.bson\" X Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X is (10, 300)\n",
      "shape of Y is (1, 300)\n"
     ]
    }
   ],
   "source": [
    "# generate some test data too just in case\n",
    "# generate training data\n",
    "# label: true distance to the car in front\n",
    "# training data: noisy ranging sensor returns\n",
    "n_test = 300\n",
    "X = []\n",
    "Y = []\n",
    "for i = 1:n_test\n",
    "    y = rand()*100 # true distance in [0,100] meters\n",
    "    # noisy measurements: Gaussian with occaisional multipath outliers\n",
    "    x = randn(n_meas) .+ y\n",
    "    if rand() < 0.2 # add an outlier to this batch of measurements \n",
    "        x[rand(1:n_meas)] *= 2\n",
    "    end\n",
    "    push!(X, x)\n",
    "    push!(Y, y)\n",
    "end\n",
    "X = hcat(X...)\n",
    "println(\"shape of X is \", size(X))\n",
    "Y = reshape(Y, 1, n_test)\n",
    "println(\"shape of Y is \", size(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save \"test_data.bson\" X Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
